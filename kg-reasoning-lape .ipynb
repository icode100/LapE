{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Installations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:30.318077Z",
                    "iopub.status.busy": "2025-03-20T15:27:30.317660Z",
                    "iopub.status.idle": "2025-03-20T15:27:34.563921Z",
                    "shell.execute_reply": "2025-03-20T15:27:34.563169Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:30.318044Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensorboardX installed.\n"
                    ]
                }
            ],
            "source": [
                "# package installer\n",
                "import importlib.util\n",
                "import subprocess\n",
                "\n",
                "def install_if_missing(package_name):\n",
                "    if importlib.util.find_spec(package_name) is None:\n",
                "        subprocess.check_call([\"pip\", \"install\", package_name])\n",
                "        print(f\"{package_name} installed.\")\n",
                "    else:\n",
                "        print(f\"{package_name} is already installed.\")\n",
                "install_if_missing(\"tensorboardX\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dataloader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:34.565325Z",
                    "iopub.status.busy": "2025-03-20T15:27:34.565029Z",
                    "iopub.status.idle": "2025-03-20T15:27:36.108717Z",
                    "shell.execute_reply": "2025-03-20T15:27:36.108061Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:34.565292Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "from __future__ import absolute_import\n",
                "from __future__ import division\n",
                "from __future__ import print_function\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "from torch.utils.data import Dataset\n",
                "import numpy as np\n",
                "import random\n",
                "import torch\n",
                "import time\n",
                "\n",
                "\n",
                "def list2tuple(l):\n",
                "    return tuple(list2tuple(x) if type(x) == list else x for x in l)\n",
                "\n",
                "\n",
                "def tuple2list(t):\n",
                "    return list(tuple2list(x) if type(x) == tuple else x for x in t)\n",
                "\n",
                "\n",
                "flatten = lambda l: sum(map(flatten, l), []) if isinstance(l, tuple) else [l]\n",
                "\n",
                "\n",
                "def parse_time():\n",
                "    return time.strftime(\"%Y.%m.%d-%H:%M:%S\", time.localtime())\n",
                "\n",
                "\n",
                "def set_global_seed(seed):\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    random.seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "\n",
                "def eval_tuple(arg_return):\n",
                "    \"\"\"Evaluate a tuple string into a tuple.\"\"\"\n",
                "    if type(arg_return) == tuple:\n",
                "        return arg_return\n",
                "    if arg_return[0] not in [\"(\", \"[\"]:\n",
                "        arg_return = eval(arg_return)\n",
                "    else:\n",
                "        splitted = arg_return[1:-1].split(\",\")\n",
                "        List = []\n",
                "        for item in splitted:\n",
                "            try:\n",
                "                item = eval(item)\n",
                "            except:\n",
                "                pass\n",
                "            if item == \"\":\n",
                "                continue\n",
                "            List.append(item)\n",
                "        arg_return = tuple(List)\n",
                "    return arg_return\n",
                "\n",
                "\n",
                "def flatten_query(queries):\n",
                "    \"\"\"assign query structure to each sample\"\"\"\n",
                "    all_queries = []\n",
                "    for query_structure in queries:\n",
                "        tmp_queries = list(queries[query_structure])\n",
                "        all_queries.extend([(query, query_structure) for query in tmp_queries])\n",
                "    return all_queries\n",
                "\n",
                "\n",
                "class TestDataset(Dataset):\n",
                "    def __init__(self, queries, nentity, nrelation):\n",
                "        # queries is a list of (query, query_structure) pairs\n",
                "        self.len = len(queries)\n",
                "        self.queries = queries\n",
                "        self.nentity = nentity\n",
                "        self.nrelation = nrelation\n",
                "\n",
                "    def __len__(self):\n",
                "        return self.len\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        query = self.queries[idx][0]\n",
                "        query_structure = self.queries[idx][1]\n",
                "        negative_sample = torch.LongTensor(range(self.nentity))\n",
                "        return negative_sample, flatten(query), query, query_structure\n",
                "\n",
                "    @staticmethod\n",
                "    def collate_fn(data):\n",
                "        negative_sample = torch.stack([_[0] for _ in data], dim=0)\n",
                "        query = [_[1] for _ in data]\n",
                "        query_unflatten = [_[2] for _ in data]\n",
                "        query_structure = [_[3] for _ in data]\n",
                "        return negative_sample, query, query_unflatten, query_structure\n",
                "\n",
                "\n",
                "class TrainDataset(Dataset):\n",
                "    def __init__(self, queries, nentity, nrelation, negative_sample_size, answer):\n",
                "        # queries is a list of (query, query_structure) pairs\n",
                "        self.len = len(queries)\n",
                "        self.queries = queries\n",
                "        self.nentity = nentity\n",
                "        self.nrelation = nrelation\n",
                "        self.negative_sample_size = negative_sample_size\n",
                "        self.count = self.count_frequency(queries, answer)\n",
                "        self.answer = answer\n",
                "\n",
                "    def __len__(self):\n",
                "        return self.len\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        query = self.queries[idx][0]\n",
                "        query_structure = self.queries[idx][1]\n",
                "        tail = np.random.choice(list(self.answer[query]))\n",
                "        subsampling_weight = self.count[query]\n",
                "        subsampling_weight = torch.sqrt(1 / torch.Tensor([subsampling_weight]))\n",
                "        negative_sample_list = []\n",
                "        negative_sample_size = 0\n",
                "        while negative_sample_size < self.negative_sample_size:\n",
                "            negative_sample = np.random.randint(self.nentity, size=self.negative_sample_size * 2)\n",
                "            mask = np.in1d(\n",
                "                negative_sample,\n",
                "                self.answer[query],\n",
                "                assume_unique=True,\n",
                "                invert=True\n",
                "            )\n",
                "            negative_sample = negative_sample[mask]\n",
                "            negative_sample_list.append(negative_sample)\n",
                "            negative_sample_size += negative_sample.size\n",
                "        negative_sample = np.concatenate(negative_sample_list)[:self.negative_sample_size]\n",
                "        negative_sample = torch.from_numpy(negative_sample)\n",
                "        positive_sample = torch.LongTensor([tail])\n",
                "        return positive_sample, negative_sample, subsampling_weight, flatten(query), query_structure\n",
                "\n",
                "    @staticmethod\n",
                "    def collate_fn(data):\n",
                "        positive_sample = torch.cat([_[0] for _ in data], dim=0)\n",
                "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
                "        subsample_weight = torch.cat([_[2] for _ in data], dim=0)\n",
                "        query = [_[3] for _ in data]\n",
                "        query_structure = [_[4] for _ in data]\n",
                "        return positive_sample, negative_sample, subsample_weight, query, query_structure\n",
                "\n",
                "    @staticmethod\n",
                "    def count_frequency(queries, answer, start=4):\n",
                "        count = {}\n",
                "        for query, qtype in queries:\n",
                "            count[query] = start + len(answer[query])\n",
                "        return count\n",
                "\n",
                "\n",
                "class SingledirectionalOneShotIterator(object):\n",
                "    def __init__(self, dataloader):\n",
                "        self.iterator = self.one_shot_iterator(dataloader)\n",
                "        self.step = 0\n",
                "\n",
                "    def __next__(self):\n",
                "        self.step += 1\n",
                "        data = next(self.iterator)\n",
                "        return data\n",
                "\n",
                "    @staticmethod\n",
                "    def one_shot_iterator(dataloader):\n",
                "        while True:\n",
                "            for data in dataloader:\n",
                "                yield data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Models\n",
                "\n",
                "### **Formulating Logical Operations Using the Laplace Distribution with Reparameterization**  \n",
                "\n",
                "Since we are using **Laplace-distributed embeddings** with the **Reparameterization Trick**, we need to define how **basic logical operations (conjunction, disjunction, negation, and projection)** work in this probabilistic embedding space.  \n",
                "\n",
                "Each entity and relation is represented as a **Laplace-distributed random variable**:  \n",
                "\n",
                "$$E = \\mathcal{L}(\\mu, b)$$  \n",
                "where:  \n",
                "- $\\mu$ is the **mean** embedding.  \n",
                "- $b$ is the **scale (uncertainty)**.  \n",
                "- We sample as:  \n",
                "  $$z = -\\text{sign}(u) \\cdot \\log(1 - 2|u|), \\quad u \\sim \\text{Uniform}(-0.5, 0.5)$$  \n",
                "  $$x = \\mu + b \\cdot z$$  \n",
                "\n",
                "\n",
                "\n",
                "\n",
                "## **4. Negation Operation (NOT Operation)**  \n",
                "To negate an entity embedding $\\mathcal{L}(E) = (\\mu, b)$, we define:\n",
                "\n",
                "$$\\mathcal{L}(\\neg E) = \\mathcal{L}(-\\mu, b + \\lambda)$$  \n",
                "\n",
                "🔹 **Intuition**:  \n",
                "- The **mean is negated** to push it in the opposite direction in embedding space.  \n",
                "- The **uncertainty increases by $\\lambda$** (a tunable hyperparameter) to reflect the fact that negation introduces **more ambiguity**.  \n",
                "\n",
                "📌 **Example Use Case**:  \n",
                "- **\"Not a city\"** should **move the embedding away** from the \"city\" space while increasing uncertainty.  \n",
                "\n",
                "---\n",
                "\n",
                "## **5. Difference Operation (Set Difference)**  \n",
                "If we want to express **\"A but not B\"**, we can define:\n",
                "\n",
                "$$\\mathcal{L}(E_1 - E_2) = \\mathcal{L}(\\mu_1, b_1 + b_2)$$  \n",
                "\n",
                "🔹 **Intuition**:  \n",
                "- The **mean stays the same**, but the **uncertainty increases**, since we are removing some knowledge.  \n",
                "\n",
                "📌 **Example Use Case**:  \n",
                "- **\"Cities in Europe but NOT in France\"** should **increase uncertainty** about which cities qualify.  \n",
                "\n",
                "---\n",
                "\n",
                "## **6. Comparison with Other Embedding Approaches**  \n",
                "| **Operation** | **Laplace-based Formulation** | **GammaE (Gamma Distribution)** | **BetaE (Beta Distribution)** |\n",
                "|--------------|----------------------------|--------------------------|--------------------------|\n",
                "| **Projection** | $\\mu_h + \\mu_r, b_h + b_r$ | $\\alpha_r \\cdot \\alpha_h, \\beta_r \\cdot \\beta_h$ | No probabilistic interpretation |\n",
                "| **Conjunction (AND)** | Weighted mean, reduced uncertainty | Min of Gamma parameters | Min of Beta parameters |\n",
                "| **Disjunction (OR)** | Mean of means, increased uncertainty | Max of Gamma parameters | Max of Beta parameters |\n",
                "| **Negation (NOT)** | $-\\mu, b + \\lambda$ | Inversion of shape parameters | No well-defined rule |\n",
                "\n",
                "📌 **Why Laplace?**  \n",
                "- More interpretable than **Gamma/Beta**, since it models **absolute differences**.  \n",
                "- **Smooth reparameterization** avoids numerical issues.  \n",
                "- **Handles logical queries better** than traditional deterministic embeddings.\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:36.111092Z",
                    "iopub.status.busy": "2025-03-20T15:27:36.110621Z",
                    "iopub.status.idle": "2025-03-20T15:27:36.115702Z",
                    "shell.execute_reply": "2025-03-20T15:27:36.114818Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:36.111054Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# imports \n",
                "from __future__ import absolute_import\n",
                "from __future__ import division\n",
                "from __future__ import print_function\n",
                "\n",
                "import logging\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "import random\n",
                "import pickle\n",
                "import math\n",
                "import collections\n",
                "import itertools\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Conjunction Operation (Intersection of Two Entities)**  \n",
                "For two Laplace embeddings $\\mathcal{L}(E_1) = (\\mu_1, b_1)$ and $\\mathcal{L}(E_2) = (\\mu_2, b_2)$, their **conjunction (AND operation)** is:\n",
                "\n",
                "$$\\mathcal{L}(E_1 \\cap E_2) = \\mathcal{L}\\left(\\frac{b_1 \\mu_2 + b_2 \\mu_1}{b_1 + b_2}, \\frac{b_1 b_2}{b_1 + b_2}\\right)$$  \n",
                "\n",
                "🔹 **Intuition**:  \n",
                "- The **new mean** is a **weighted average** based on uncertainty (more confident entities have more influence).  \n",
                "- The **new scale (uncertainty) decreases**, meaning intersection leads to **more confident knowledge**.  \n",
                "\n",
                "📌 **Example Use Case**:  \n",
                "- **\"Paris is in France\"** $\\cap$ **\"Paris is the capital of a country\"**  \n",
                "- The result will be a **more certain** embedding for **Paris → Country**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:36.117148Z",
                    "iopub.status.busy": "2025-03-20T15:27:36.116865Z",
                    "iopub.status.idle": "2025-03-20T15:27:36.128720Z",
                    "shell.execute_reply": "2025-03-20T15:27:36.127804Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:36.117128Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "class LaplaceIntersection(nn.Module):\n",
                "    def __init__(self, dim):\n",
                "        super(LaplaceIntersection, self).__init__()\n",
                "        self.dim = dim\n",
                "        # Linear layers for computing attention over mean (mu)\n",
                "        self.layer_mu1 = nn.Linear(self.dim * 2, self.dim)\n",
                "        self.layer_mu2 = nn.Linear(self.dim, self.dim)\n",
                "        # Linear layers for computing attention over scale (b)\n",
                "        self.layer_b1 = nn.Linear(self.dim * 2, self.dim)\n",
                "        self.layer_b2 = nn.Linear(self.dim, self.dim)\n",
                "\n",
                "        nn.init.xavier_uniform_(self.layer_mu1.weight)\n",
                "        nn.init.xavier_uniform_(self.layer_mu2.weight)\n",
                "        nn.init.xavier_uniform_(self.layer_b1.weight)\n",
                "        nn.init.xavier_uniform_(self.layer_b2.weight)\n",
                "\n",
                "    def forward(self, mu_embeddings, b_embeddings):\n",
                "        \"\"\"\n",
                "        Implements the intersection (AND) operation on Laplace embeddings.\n",
                "        \n",
                "        Inputs:\n",
                "          - mu_embeddings: Tensor of shape (num_conj, batch_size, dim) representing means.\n",
                "          - b_embeddings: Tensor of shape (num_conj, batch_size, dim) representing scale parameters.\n",
                "          \n",
                "        Outputs:\n",
                "          - mu_out: Mean of the resulting intersection embedding.\n",
                "          - b_out: Scale (uncertainty) of the resulting intersection embedding.\n",
                "        \"\"\"\n",
                "        # Concatenate mean and scale embeddings along last dimension\n",
                "        all_embeddings = torch.cat([mu_embeddings, b_embeddings], dim=-1)\n",
                "\n",
                "        # Compute attention for mu (mean)\n",
                "        layer_mu = F.relu(self.layer_mu1(all_embeddings))  \n",
                "        attention_mu = F.softmax(self.layer_mu2(layer_mu), dim=0)  \n",
                "\n",
                "        # Compute attention for b (scale)\n",
                "        layer_b = F.relu(self.layer_b1(all_embeddings))  \n",
                "        attention_b = F.softmax(self.layer_b2(layer_b), dim=0)  \n",
                "\n",
                "        # Compute new mean using weighted sum (weighted by uncertainty)\n",
                "        mu_out = torch.sum(attention_mu * mu_embeddings, dim=0)\n",
                "        # Compute new scale using harmonic mean-like aggregation\n",
                "        b_out = torch.sum(attention_b * b_embeddings, dim=0)  \n",
                "\n",
                "        # Clamping the scale to avoid extreme values\n",
                "        b_out = torch.clamp(b_out, min=1e-4, max=1.0)\n",
                "\n",
                "        return mu_out, b_out\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Disjunction Operation (Union of Two Entities)**  \n",
                "For **OR operation** (choosing between two possible entities):\n",
                "\n",
                "$$\\mathcal{L}(E_1 \\cup E_2) = \\mathcal{L}\\left(\\frac{\\mu_1 + \\mu_2}{2}, b_1 + b_2\\right)$$  \n",
                "\n",
                "🔹 **Intuition**:  \n",
                "- The **new mean is the midpoint** between the two possibilities.  \n",
                "- The **uncertainty increases**, since the union introduces more ambiguity.  \n",
                "\n",
                "📌 **Example Use Case**:  \n",
                "- **\"The capital of Canada is Ottawa OR Toronto\"**  \n",
                "- The model represents both possibilities with **higher uncertainty**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:36.129914Z",
                    "iopub.status.busy": "2025-03-20T15:27:36.129620Z",
                    "iopub.status.idle": "2025-03-20T15:27:36.143296Z",
                    "shell.execute_reply": "2025-03-20T15:27:36.142421Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:36.129882Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "class LaplaceUnion(nn.Module):\n",
                "    def __init__(self, dim, projection_regularizer, drop):\n",
                "        super(LaplaceUnion, self).__init__()\n",
                "        self.dim = dim\n",
                "        # Layers for mean (mu)\n",
                "        self.layer_mu1 = nn.Linear(self.dim * 2, self.dim)\n",
                "        self.layer_mu2 = nn.Linear(self.dim, self.dim // 2)\n",
                "        self.layer_mu3 = nn.Linear(self.dim // 2, self.dim)\n",
                "        # Layers for scale (b)\n",
                "        self.layer_b1 = nn.Linear(self.dim * 2, self.dim)\n",
                "        self.layer_b2 = nn.Linear(self.dim, self.dim // 2)\n",
                "        self.layer_b3 = nn.Linear(self.dim // 2, self.dim)\n",
                "\n",
                "        self.projection_regularizer = projection_regularizer\n",
                "        self.drop = nn.Dropout(p=drop)\n",
                "\n",
                "        nn.init.xavier_uniform_(self.layer_mu1.weight)\n",
                "        nn.init.xavier_uniform_(self.layer_mu2.weight)\n",
                "        nn.init.xavier_uniform_(self.layer_mu3.weight)\n",
                "        nn.init.xavier_uniform_(self.layer_b1.weight)\n",
                "        nn.init.xavier_uniform_(self.layer_b2.weight)\n",
                "        nn.init.xavier_uniform_(self.layer_b3.weight)\n",
                "\n",
                "    def forward(self, mu_embeddings, b_embeddings):\n",
                "        \"\"\"\n",
                "        Implements the union (OR) operation on Laplace embeddings.\n",
                "        Inputs:\n",
                "          - mu_embeddings: Tensor of shape (num_disj, batch_size, dim) for means.\n",
                "          - b_embeddings: Tensor of shape (num_disj, batch_size, dim) for scale parameters.\n",
                "          \n",
                "        Outputs:\n",
                "          - mu_out: Mean of the resulting union embedding.\n",
                "          - b_out: Scale (uncertainty) of the resulting union embedding.\n",
                "        \"\"\"\n",
                "        # Concatenate means and scales along the last dimension\n",
                "        all_embeddings = torch.cat([mu_embeddings, b_embeddings], dim=-1)\n",
                "        \n",
                "        # Compute attention for mu (mean)\n",
                "        layer_mu = F.relu(self.layer_mu1(all_embeddings))  \n",
                "        layer_mu = F.relu(self.layer_mu2(layer_mu))\n",
                "        attention_mu = F.softmax(self.drop(self.layer_mu3(layer_mu)), dim=0)  \n",
                "\n",
                "        # Compute attention for b (scale)\n",
                "        layer_b = F.relu(self.layer_b1(all_embeddings))  \n",
                "        layer_b = F.relu(self.layer_b2(layer_b))\n",
                "        attention_b = F.softmax(self.drop(self.layer_b3(layer_b)), dim=0)  \n",
                "\n",
                "        # Compute new mean and scale\n",
                "        mu_out = torch.sum(attention_mu * mu_embeddings, dim=0)  # Average of means\n",
                "        b_out = torch.sum(b_embeddings, dim=0)  # Sum of uncertainties\n",
                "\n",
                "        # Clamping the scale to avoid extreme values\n",
                "        b_out = torch.clamp(b_out, min=1e-4, max=1.0)\n",
                "\n",
                "        return mu_out, b_out\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Projection Operation (Entity + Relation → New Entity)**  \n",
                "Given a **head entity** $h$ and a **relation** $r$, the new projected embedding $t$ is:\n",
                "\n",
                "$$\\mathcal{L}(h + r) = \\mathcal{L}(\\mu_h + \\mu_r, b_h + b_r)$$  \n",
                "\n",
                "🔹 **Intuition**:  \n",
                "- The **mean vectors add** because relations shift entity embeddings.  \n",
                "- The **uncertainty (scale) also adds**, ensuring that uncertainty accumulates through multiple reasoning steps.  \n",
                "- This allows multi-hop reasoning while **keeping track of uncertainty**.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:36.144357Z",
                    "iopub.status.busy": "2025-03-20T15:27:36.144073Z",
                    "iopub.status.idle": "2025-03-20T15:27:36.160433Z",
                    "shell.execute_reply": "2025-03-20T15:27:36.159872Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:36.144330Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class LaplaceProjection(nn.Module):\n",
                "    def __init__(self, entity_dim, relation_dim, hidden_dim, projection_regularizer, num_layers):\n",
                "        super(LaplaceProjection, self).__init__()\n",
                "        self.entity_dim = entity_dim\n",
                "        self.relation_dim = relation_dim\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        # Neural network for processing the mean (mu)\n",
                "        self.layer_mu1 = nn.Linear(self.entity_dim + self.relation_dim, self.hidden_dim)  \n",
                "        self.layer_mu0 = nn.Linear(self.hidden_dim, self.entity_dim)  # Final layer\n",
                "        \n",
                "        # Neural network for processing the scale (b)\n",
                "        self.layer_b1 = nn.Linear(self.entity_dim + self.relation_dim, self.hidden_dim)  \n",
                "        self.layer_b0 = nn.Linear(self.hidden_dim, self.entity_dim)  \n",
                "\n",
                "        # Additional layers for deeper networks\n",
                "        for nl in range(2, num_layers + 1):\n",
                "            setattr(self, f\"layer_mu{nl}\", nn.Linear(self.hidden_dim, self.hidden_dim))\n",
                "            setattr(self, f\"layer_b{nl}\", nn.Linear(self.hidden_dim, self.hidden_dim))\n",
                "\n",
                "        # Xavier Initialization\n",
                "        for nl in range(1, num_layers + 1):\n",
                "            nn.init.xavier_uniform_(getattr(self, f\"layer_mu{nl}\").weight)\n",
                "            nn.init.xavier_uniform_(getattr(self, f\"layer_b{nl}\").weight)\n",
                "\n",
                "        self.projection_regularizer = projection_regularizer\n",
                "\n",
                "    def forward(self, mu_embedding, b_embedding, mu_embedding_r, b_embedding_r):\n",
                "        \"\"\"\n",
                "        Implements the projection operation: moving from one entity to another via a relation.\n",
                "        \n",
                "        Inputs:\n",
                "          - mu_embedding: Mean of entity embeddings.\n",
                "          - b_embedding: Scale (uncertainty) of entity embeddings.\n",
                "          - mu_embedding_r: Mean of relation embeddings.\n",
                "          - b_embedding_r: Scale (uncertainty) of relation embeddings.\n",
                "        \n",
                "        Outputs:\n",
                "          - mu_out: Projected mean embedding.\n",
                "          - b_out: Projected scale (uncertainty).\n",
                "        \"\"\"\n",
                "        # Concatenate entity and relation embeddings\n",
                "        x_mu = torch.cat([mu_embedding, mu_embedding_r], dim=-1)\n",
                "        x_b = torch.cat([b_embedding, b_embedding_r], dim=-1)\n",
                "\n",
                "        # Pass through deep network for mu (mean)\n",
                "        for nl in range(1, self.num_layers + 1):\n",
                "            x_mu = F.relu(getattr(self, f\"layer_mu{nl}\")(x_mu))\n",
                "        mu_out = self.layer_mu0(x_mu)\n",
                "        mu_out = self.projection_regularizer(mu_out)\n",
                "\n",
                "        # Pass through deep network for b (scale)\n",
                "        for nl in range(1, self.num_layers + 1):\n",
                "            x_b = F.relu(getattr(self, f\"layer_b{nl}\")(x_b))\n",
                "        b_out = self.layer_b0(x_b)\n",
                "        b_out = self.projection_regularizer(b_out)\n",
                "\n",
                "        # Enforce positivity constraint on scale (uncertainty)\n",
                "        b_out = torch.clamp(b_out, min=1e-4, max=1.0)\n",
                "\n",
                "        return mu_out, b_out\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Regularizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:36.161531Z",
                    "iopub.status.busy": "2025-03-20T15:27:36.161267Z",
                    "iopub.status.idle": "2025-03-20T15:27:36.175868Z",
                    "shell.execute_reply": "2025-03-20T15:27:36.175080Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:36.161498Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "class Regularizer():\n",
                "    def __init__(self, base_add, min_val, max_val):\n",
                "        self.base_add = base_add\n",
                "        self.min_val = min_val\n",
                "        self.max_val = max_val\n",
                "\n",
                "    def __call__(self, entity_embedding):\n",
                "        return torch.clamp(entity_embedding + self.base_add, self.min_val, self.max_val)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# KGReasoning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Distance between two entity embeddings computed using Wasserstein-1 distance between Laplace distributions.\n",
                "$$\n",
                "W_1(\\mathcal{L}_1, \\mathcal{L}_2) = |\\mu_1 - \\mu_2| + |b_1 - b_2|\n",
                "$$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:36.177829Z",
                    "iopub.status.busy": "2025-03-20T15:27:36.177585Z",
                    "iopub.status.idle": "2025-03-20T15:27:36.293871Z",
                    "shell.execute_reply": "2025-03-20T15:27:36.292839Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:36.177810Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "class KGReasoningLapE(nn.Module):\n",
                "    def __init__(self, nentity, nrelation, hidden_dim, gamma,\n",
                "                 geo, test_batch_size=1,\n",
                "                 box_mode=None, use_cuda=False,\n",
                "                 query_name_dict=None, beta_mode=None, gamma_mode=None, drop=0.):\n",
                "        super(KGReasoningLapE, self).__init__()\n",
                "\n",
                "        self.nentity = nentity\n",
                "        self.nrelation = nrelation\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.epsilon = 2.0\n",
                "        self.geo = geo\n",
                "        self.is_u = False\n",
                "        self.use_cuda = use_cuda\n",
                "        self.batch_entity_range = (\n",
                "            torch.arange(nentity).to(torch.float).repeat(test_batch_size, 1).cuda()\n",
                "            if self.use_cuda else torch.arange(nentity).to(torch.float).repeat(test_batch_size, 1)\n",
                "        )\n",
                "        self.query_name_dict = query_name_dict\n",
                "\n",
                "        self.gamma = nn.Parameter(torch.Tensor([gamma]), requires_grad=False)\n",
                "        self.embedding_range = nn.Parameter(\n",
                "            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]), requires_grad=False\n",
                "        )\n",
                "\n",
                "        self.entity_dim = hidden_dim\n",
                "        self.relation_dim = hidden_dim\n",
                "\n",
                "        # Each entity embedding is represented as a Laplace distribution:\n",
                "        # first half is μ (mean) and second half is b (scale/uncertainty)\n",
                "        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim * 2))\n",
                "        self.entity_regularizer = Regularizer(1, 0.15, 1e9)\n",
                "        self.projection_regularizer = Regularizer(1, 0.15, 1e9)\n",
                "\n",
                "        nn.init.uniform_(\n",
                "            tensor=self.entity_embedding,\n",
                "            a=-3 * self.embedding_range.item(),\n",
                "            b=3 * self.embedding_range.item()\n",
                "        )\n",
                "\n",
                "        # Standard relation embeddings remain for additional relation information\n",
                "        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))\n",
                "        nn.init.uniform_(\n",
                "            tensor=self.relation_embedding,\n",
                "            a=-3 * self.embedding_range.item(),\n",
                "            b=3 * self.embedding_range.item()\n",
                "        )\n",
                "\n",
                "        # For relations, we now maintain separate parameters for μ and b\n",
                "        self.mu_relation = nn.Parameter(torch.zeros(nrelation, self.relation_dim), requires_grad=True)\n",
                "        nn.init.uniform_(\n",
                "            tensor=self.mu_relation,\n",
                "            a=-3 * self.embedding_range.item(),\n",
                "            b=3 * self.embedding_range.item()\n",
                "        )\n",
                "        self.b_relation = nn.Parameter(torch.zeros(nrelation, self.relation_dim), requires_grad=True)\n",
                "        nn.init.uniform_(\n",
                "            tensor=self.b_relation,\n",
                "            a=-3 * self.embedding_range.item(),\n",
                "            b=3 * self.embedding_range.item()\n",
                "        )\n",
                "\n",
                "        self.modulus = nn.Parameter(torch.Tensor([1 * self.embedding_range.item()]), requires_grad=True)\n",
                "\n",
                "        # gamma_mode returns (hidden_dim, num_layers)\n",
                "        hidden_dim, num_layers = gamma_mode\n",
                "        # Use our Laplace-based modules for logical operations:\n",
                "        self.center_net = LaplaceIntersection(self.entity_dim)\n",
                "        self.projection_net = LaplaceProjection(self.entity_dim,\n",
                "                                                 self.relation_dim,\n",
                "                                                 hidden_dim,\n",
                "                                                 self.projection_regularizer,\n",
                "                                                 num_layers)\n",
                "        self.union_net = LaplaceUnion(self.entity_dim, self.projection_regularizer, drop)\n",
                "    def sample_laplace(self, mu, b):\n",
                "        # u ~ Uniform(-0.5, 0.5)\n",
                "        u = torch.rand_like(mu) - 0.5\n",
                "        # z ~ Laplace(0, 1)\n",
                "        z = -torch.sign(u) * torch.log(1 - 2 * torch.abs(u) + 1e-12)\n",
                "        # reparameterized sample\n",
                "        return mu + b * z\n",
                "    def embed_query_lape(self, queries, query_structure, idx):\n",
                "        '''\n",
                "        Iteratively embeds a batch of queries with the same structure using Laplace-based embeddings.\n",
                "        Each entity is represented as a Laplace distribution: (mu, b).\n",
                "        '''\n",
                "        # Special case handling (if needed)\n",
                "        if query_structure == ((('e', ('r',)), ('e', ('r', 'n'))), ('r',)):\n",
                "            aa = 1  # (dummy code, as in original)\n",
                "        \n",
                "        # Determine if the current query structure is purely relational (only 'r' and 'n')\n",
                "        all_relation_flag = True\n",
                "        for ele in query_structure[-1]:\n",
                "            if ele not in ['r', 'n']:\n",
                "                all_relation_flag = False\n",
                "                break\n",
                "\n",
                "        if all_relation_flag:\n",
                "            # Base case: query structure starts with an entity\n",
                "            if query_structure[0] == 'e':\n",
                "                ent_embedding = torch.index_select(self.entity_embedding, dim=0, index=queries[:, idx])\n",
                "                # Split the entity embedding into mean (mu) and scale (b)\n",
                "                mu_embedding, b_embedding = torch.chunk(ent_embedding, 2, dim=-1)\n",
                "                mu_embedding = self.sample_laplace(mu_embedding, b_embedding)\n",
                "                idx += 1\n",
                "            else:\n",
                "                mu_embedding, b_embedding, idx = self.embed_query_lape(queries, query_structure[0], idx)\n",
                "            \n",
                "            # Process each relation (or negation) in the current branch\n",
                "            for i in range(len(query_structure[-1])):\n",
                "                if query_structure[-1][i] == 'n':\n",
                "                # Negation: flip the sign of the mean and increase the scale\n",
                "                # Here we add a constant (0.07) to represent increased uncertainty\n",
                "                    mu_embedding = -mu_embedding\n",
                "                    b_embedding = b_embedding + 0.07\n",
                "                else:\n",
                "                # For relation traversal, use the LapE relation embeddings:\n",
                "                # Use self.mu_relation and self.b_relation instead of self.alpha_embedding and self.beta_embedding\n",
                "                    mu_r_embedding = torch.index_select(self.mu_relation, dim=0, index=queries[:, idx])\n",
                "                    b_r_embedding = torch.index_select(self.b_relation, dim=0, index=queries[:, idx])\n",
                "                    # Apply projection operation\n",
                "                    mu_embedding, b_embedding = self.projection_net(mu_embedding, b_embedding,\n",
                "                                                                 mu_r_embedding, b_r_embedding)\n",
                "                idx += 1\n",
                "\n",
                "        else:\n",
                "        # If not all relations, then we are dealing with a multi-branch (e.g., union or intersection) query.\n",
                "            if self.is_u:\n",
                "                mu_embedding_list = []\n",
                "                b_embedding_list = []\n",
                "                for i in range(len(query_structure)):\n",
                "                    mu_emb, b_emb, idx = self.embed_query_lape(queries, query_structure[i], idx)\n",
                "                    mu_embedding_list.append(mu_emb)\n",
                "                    b_embedding_list.append(b_emb)\n",
                "                mu_embedding, b_embedding = self.union_net(torch.stack(mu_embedding_list),\n",
                "                                                        torch.stack(b_embedding_list))\n",
                "            else:\n",
                "                mu_embedding_list = []\n",
                "                b_embedding_list = []\n",
                "                for i in range(len(query_structure)):\n",
                "                    mu_emb, b_emb, idx = self.embed_query_lape(queries, query_structure[i], idx)\n",
                "                    mu_embedding_list.append(mu_emb)\n",
                "                    b_embedding_list.append(b_emb)\n",
                "                mu_embedding, b_embedding = self.center_net(torch.stack(mu_embedding_list),\n",
                "                                                         torch.stack(b_embedding_list))\n",
                "\n",
                "        return mu_embedding, b_embedding, idx\n",
                "\n",
                "    def cal_logit_lape(self, entity_embedding, query_dist):\n",
                "        \"\"\"\n",
                "        Compute the logit for a query based on Laplace-distributed embeddings.\n",
                "        \n",
                "        Args:\n",
                "          entity_embedding: Tensor of shape (..., 2 * dim), where the first half is μ (mean)\n",
                "                            and the second half is b (scale).\n",
                "          query_dist: A tuple (query_mu, query_b) representing the Laplace distribution\n",
                "                      of the query. distance computed using Wasserstein-1 distance between Laplace distributions.\n",
                "        \n",
                "        Returns:\n",
                "          logit: The computed logit value, where higher values indicate higher similarity.\n",
                "        \"\"\"\n",
                "        # Split the entity embedding into mean (mu) and scale (b)\n",
                "        mu_embedding, b_embedding = torch.chunk(entity_embedding, 2, dim=-1)\n",
                "        # Unpack the query distribution (assumed to be in the same format: (mu, b))\n",
                "        query_mu, query_b = query_dist\n",
                "        query_mu = self.sample_laplace(query_mu, query_b)\n",
                "        # Compute the Wasserstein-1 distance between the Laplace distributions:\n",
                "        # Compute elementwise absolute differences for both μ and b, then sum over the embedding dimensions.\n",
                "        distance = torch.abs(mu_embedding - query_mu) + torch.abs(b_embedding - query_b)\n",
                "        distance = torch.sum(distance, dim=-1)\n",
                "        \n",
                "        # Compute the logit by subtracting the distance from the margin parameter gamma.\n",
                "        logit = self.gamma - distance\n",
                "    \n",
                "        return logit\n",
                "\n",
                "    def forward(self, positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict):\n",
                "        all_idxs, all_mu_embeddings, all_b_embeddings = [], [], []\n",
                "        all_union_idxs, all_union_mu_embeddings, all_union_b_embeddings = [], [], []\n",
                "    \n",
                "        # Loop over each query structure in the batch.\n",
                "        for query_structure in batch_queries_dict:\n",
                "            if 'u' in self.query_name_dict[query_structure] and 'DNF' in self.query_name_dict[query_structure]:\n",
                "                self.is_u = True\n",
                "                # For union queries, transform the query and then embed it.\n",
                "                mu_embedding, b_embedding, _ = \\\n",
                "                    self.embed_query_lape(self.transform_union_query(batch_queries_dict[query_structure],\n",
                "                                                                      query_structure),\n",
                "                                           self.transform_union_structure(query_structure),\n",
                "                                           0)\n",
                "                all_union_idxs.extend(batch_idxs_dict[query_structure])\n",
                "                all_union_mu_embeddings.append(mu_embedding)\n",
                "                all_union_b_embeddings.append(b_embedding)\n",
                "            else:\n",
                "                self.is_u = False\n",
                "                mu_embedding, b_embedding, _ = self.embed_query_lape(batch_queries_dict[query_structure],\n",
                "                                                                     query_structure,\n",
                "                                                                     0)\n",
                "                all_idxs.extend(batch_idxs_dict[query_structure])\n",
                "                all_mu_embeddings.append(mu_embedding)\n",
                "                all_b_embeddings.append(b_embedding)\n",
                "    \n",
                "        # Form the Laplace distributions for non-union queries as (mu, b) tuples.\n",
                "        if len(all_mu_embeddings) > 0:\n",
                "            all_mu_embeddings = torch.cat(all_mu_embeddings, dim=0).unsqueeze(1)\n",
                "            all_b_embeddings = torch.cat(all_b_embeddings, dim=0).unsqueeze(1)\n",
                "            all_dists = (all_mu_embeddings, all_b_embeddings)\n",
                "        # For union queries.\n",
                "        if len(all_union_mu_embeddings) > 0:\n",
                "            all_union_mu_embeddings = torch.cat(all_union_mu_embeddings, dim=0).unsqueeze(1)\n",
                "            all_union_b_embeddings = torch.cat(all_union_b_embeddings, dim=0).unsqueeze(1)\n",
                "            all_union_dists = (all_union_mu_embeddings, all_union_b_embeddings)\n",
                "            \n",
                "        if subsampling_weight is not None:\n",
                "            subsampling_weight = subsampling_weight[all_idxs + all_union_idxs]\n",
                "    \n",
                "        # Process positive samples.\n",
                "        if positive_sample is not None:\n",
                "            if len(all_mu_embeddings) > 0:\n",
                "                positive_sample_regular = positive_sample[all_idxs]\n",
                "                positive_embedding = self.entity_regularizer(\n",
                "                    torch.index_select(self.entity_embedding, dim=0, index=positive_sample_regular).unsqueeze(1))\n",
                "                positive_logit = self.cal_logit_lape(positive_embedding, all_dists)\n",
                "            else:\n",
                "                positive_logit = torch.Tensor([]).to(self.entity_embedding.device)\n",
                "    \n",
                "            if len(all_union_mu_embeddings) > 0:\n",
                "                positive_sample_union = positive_sample[all_union_idxs]\n",
                "                positive_embedding = self.entity_regularizer(\n",
                "                    torch.index_select(self.entity_embedding, dim=0, index=positive_sample_union).unsqueeze(1))\n",
                "                positive_union_logit = self.cal_logit_lape(positive_embedding, all_union_dists)\n",
                "            else:\n",
                "                positive_union_logit = torch.Tensor([]).to(self.entity_embedding.device)\n",
                "            positive_logit = torch.cat([positive_logit, positive_union_logit], dim=0)\n",
                "        else:\n",
                "            positive_logit = None\n",
                "    \n",
                "        # Process negative samples.\n",
                "        if negative_sample is not None:\n",
                "            if len(all_mu_embeddings) > 0:\n",
                "                negative_sample_regular = negative_sample[all_idxs]\n",
                "                batch_size, negative_size = negative_sample_regular.shape\n",
                "                negative_embedding = self.entity_regularizer(\n",
                "                    torch.index_select(self.entity_embedding, dim=0, index=negative_sample_regular.view(-1))\n",
                "                    .view(batch_size, negative_size, -1)\n",
                "                )\n",
                "                negative_logit = self.cal_logit_lape(negative_embedding, all_dists)\n",
                "            else:\n",
                "                negative_logit = torch.Tensor([]).to(self.entity_embedding.device)\n",
                "    \n",
                "            if len(all_union_mu_embeddings) > 0:\n",
                "                negative_sample_union = negative_sample[all_union_idxs]\n",
                "                batch_size, negative_size = negative_sample_union.shape\n",
                "                negative_embedding = self.entity_regularizer(\n",
                "                    torch.index_select(self.entity_embedding, dim=0, index=negative_sample_union.view(-1))\n",
                "                    .view(batch_size, negative_size, -1)\n",
                "                )\n",
                "                negative_union_logit = self.cal_logit_lape(negative_embedding, all_union_dists)\n",
                "            else:\n",
                "                negative_union_logit = torch.Tensor([]).to(self.entity_embedding.device)\n",
                "            negative_logit = torch.cat([negative_logit, negative_union_logit], dim=0)\n",
                "        else:\n",
                "            negative_logit = None\n",
                "    \n",
                "        return positive_logit, negative_logit, subsampling_weight, all_idxs + all_union_idxs\n",
                "    def transform_union_query(self, queries, query_structure):\n",
                "        # For union queries, the transformation remains the same\n",
                "        # regardless of whether we use Gamma or Laplace embeddings.\n",
                "        if self.query_name_dict[query_structure] == '2u-DNF':\n",
                "            queries = queries[:, :-1]\n",
                "        elif self.query_name_dict[query_structure] == 'up-DNF':\n",
                "            queries = torch.cat([queries[:, :4], queries[:, 5:6]], dim=1)\n",
                "        return queries\n",
                "    \n",
                "    def transform_union_structure(self, query_structure):\n",
                "        # The union structure mapping is identical for LapE.\n",
                "        if self.query_name_dict[query_structure] == '2u-DNF':\n",
                "            return (('e', ('r',)), ('e', ('r',)))\n",
                "        elif self.query_name_dict[query_structure] == 'up-DNF':\n",
                "            return ((('e', ('r',)), ('e', ('r',))), ('r',))\n",
                "\n",
                "    @staticmethod\n",
                "    def train_step(model, optimizer, train_iterator, args, step):\n",
                "        model.train()\n",
                "        optimizer.zero_grad()\n",
                "    \n",
                "        positive_sample, negative_sample, subsampling_weight, batch_queries, query_structures = next(train_iterator)\n",
                "        batch_queries_dict = collections.defaultdict(list)\n",
                "        batch_idxs_dict = collections.defaultdict(list)\n",
                "        for i, query in enumerate(batch_queries):\n",
                "            batch_queries_dict[query_structures[i]].append(query)\n",
                "            batch_idxs_dict[query_structures[i]].append(i)\n",
                "        for query_structure in batch_queries_dict:\n",
                "            if args.cuda:\n",
                "                batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure]).cuda()\n",
                "            else:\n",
                "                batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure])\n",
                "        if args.cuda:\n",
                "            positive_sample = positive_sample.cuda()\n",
                "            negative_sample = negative_sample.cuda()\n",
                "            subsampling_weight = subsampling_weight.cuda()\n",
                "    \n",
                "        # Call the LapE model's forward function, which returns positive and negative logits computed via Laplace embeddings.\n",
                "        positive_logit, negative_logit, subsampling_weight, _ = model(\n",
                "            positive_sample, negative_sample, subsampling_weight, batch_queries_dict, batch_idxs_dict\n",
                "        )\n",
                "    \n",
                "        # Compute the ranking loss using logsigmoid. Note that a higher logit means a better match.\n",
                "        negative_score = F.logsigmoid(-negative_logit).mean(dim=1)\n",
                "        positive_score = F.logsigmoid(positive_logit).squeeze(dim=1)\n",
                "        positive_sample_loss = - (subsampling_weight * positive_score).sum()\n",
                "        negative_sample_loss = - (subsampling_weight * negative_score).sum()\n",
                "        positive_sample_loss /= subsampling_weight.sum()\n",
                "        negative_sample_loss /= subsampling_weight.sum()\n",
                "    \n",
                "        loss = (positive_sample_loss + negative_sample_loss) / 2\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        log = {\n",
                "            'positive_sample_loss': positive_sample_loss.item(),\n",
                "            'negative_sample_loss': negative_sample_loss.item(),\n",
                "            'loss': loss.item(),\n",
                "        }\n",
                "        return log\n",
                "    \n",
                "    @staticmethod\n",
                "    def test_step(model, easy_answers, hard_answers, args, test_dataloader, query_name_dict, save_result=False,\n",
                "                  save_str=\"\", save_empty=False):\n",
                "        model.eval()\n",
                "        step = 0\n",
                "        total_steps = len(test_dataloader)\n",
                "        logs = collections.defaultdict(list)\n",
                "    \n",
                "        with torch.no_grad():\n",
                "            for negative_sample, queries, queries_unflatten, query_structures in tqdm(\n",
                "                    test_dataloader, disable=not args.print_on_screen):\n",
                "                batch_queries_dict = collections.defaultdict(list)\n",
                "                batch_idxs_dict = collections.defaultdict(list)\n",
                "                for i, query in enumerate(queries):\n",
                "                    batch_queries_dict[query_structures[i]].append(query)\n",
                "                    batch_idxs_dict[query_structures[i]].append(i)\n",
                "                for query_structure in batch_queries_dict:\n",
                "                    if args.cuda:\n",
                "                        batch_queries_dict[query_structure] = torch.LongTensor(\n",
                "                            batch_queries_dict[query_structure]).cuda()\n",
                "                    else:\n",
                "                        batch_queries_dict[query_structure] = torch.LongTensor(batch_queries_dict[query_structure])\n",
                "                if args.cuda:\n",
                "                    negative_sample = negative_sample.cuda()\n",
                "    \n",
                "                # Call our LapE model's forward function (which now returns logits computed using Laplace embeddings)\n",
                "                _, negative_logit, _, idxs = model(None, negative_sample, None, batch_queries_dict, batch_idxs_dict)\n",
                "                queries_unflatten = [queries_unflatten[i] for i in idxs]\n",
                "                query_structures = [query_structures[i] for i in idxs]\n",
                "                argsort = torch.argsort(negative_logit, dim=1, descending=True)\n",
                "                ranking = argsort.clone().to(torch.float)\n",
                "                if len(argsort) == args.test_batch_size:  # reuse batch_entity_range if possible\n",
                "                    ranking = ranking.scatter_(1, argsort, model.batch_entity_range)\n",
                "                else:\n",
                "                    if args.cuda:\n",
                "                        ranking = ranking.scatter_(1, argsort,\n",
                "                                                    torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1).cuda())\n",
                "                    else:\n",
                "                        ranking = ranking.scatter_(1, argsort,\n",
                "                                                    torch.arange(model.nentity).to(torch.float).repeat(argsort.shape[0], 1))\n",
                "                for idx, (i, query, query_structure) in enumerate(zip(argsort[:, 0], queries_unflatten, query_structures)):\n",
                "                    hard_answer = hard_answers[query]\n",
                "                    easy_answer = easy_answers[query]\n",
                "                    num_hard = len(hard_answer)\n",
                "                    num_easy = len(easy_answer)\n",
                "                    assert len(hard_answer.intersection(easy_answer)) == 0\n",
                "                    cur_ranking = ranking[idx, list(easy_answer) + list(hard_answer)]\n",
                "                    cur_ranking, indices = torch.sort(cur_ranking)\n",
                "                    masks = indices >= num_easy\n",
                "                    if args.cuda:\n",
                "                        answer_list = torch.arange(num_hard + num_easy).to(torch.float).cuda()\n",
                "                    else:\n",
                "                        answer_list = torch.arange(num_hard + num_easy).to(torch.float)\n",
                "                    cur_ranking = cur_ranking - answer_list + 1  # filtered setting\n",
                "                    cur_ranking = cur_ranking[masks]  # only take indices that belong to the hard answers\n",
                "                        \n",
                "                    mrr = torch.mean(1. / cur_ranking).item()\n",
                "                    h1 = torch.mean((cur_ranking <= 1).to(torch.float)).item()\n",
                "                    h3 = torch.mean((cur_ranking <= 3).to(torch.float)).item()\n",
                "                    h10 = torch.mean((cur_ranking <= 10).to(torch.float)).item()\n",
                "    \n",
                "                    logs[query_structure].append({\n",
                "                        'MRR': mrr,\n",
                "                        'HITS1': h1,\n",
                "                        'HITS3': h3,\n",
                "                        'HITS10': h10,\n",
                "                        'num_hard_answer': num_hard,\n",
                "                    })\n",
                "    \n",
                "                if step % args.test_log_steps == 0:\n",
                "                    logging.info('Evaluating the model... (%d/%d)' % (step, total_steps))\n",
                "                step += 1\n",
                "    \n",
                "        metrics = collections.defaultdict(lambda: collections.defaultdict(int))\n",
                "        for query_structure in logs:\n",
                "            for metric in logs[query_structure][0].keys():\n",
                "                if metric in ['num_hard_answer']:\n",
                "                    continue\n",
                "                metrics[query_structure][metric] = sum([log[metric] for log in logs[query_structure]]) / len(logs[query_structure])\n",
                "            metrics[query_structure]['num_queries'] = len(logs[query_structure])\n",
                "    \n",
                "        return metrics\n",
                "    \n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Main runner"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:36.295227Z",
                    "iopub.status.busy": "2025-03-20T15:27:36.295003Z",
                    "iopub.status.idle": "2025-03-20T15:27:47.379664Z",
                    "shell.execute_reply": "2025-03-20T15:27:47.378890Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:36.295210Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# imports\n",
                "from __future__ import absolute_import\n",
                "from __future__ import division\n",
                "from __future__ import print_function\n",
                "import _thread\n",
                "import argparse\n",
                "import json\n",
                "import logging\n",
                "import os\n",
                "import random\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from tensorboardX import SummaryWriter\n",
                "import time\n",
                "import pickle\n",
                "from collections import defaultdict\n",
                "from tqdm import tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2025-03-20T15:27:47.380979Z",
                    "iopub.status.busy": "2025-03-20T15:27:47.380524Z",
                    "iopub.status.idle": "2025-03-20T15:27:47.421864Z",
                    "shell.execute_reply": "2025-03-20T15:27:47.421086Z",
                    "shell.execute_reply.started": "2025-03-20T15:27:47.380956Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "query_name_dict = {('e', ('r',)): '1p',\n",
                "                   ('e', ('r', 'r')): '2p',\n",
                "                   ('e', ('r', 'r', 'r')): '3p',\n",
                "                   (('e', ('r',)), ('e', ('r',))): '2i',\n",
                "                   (('e', ('r',)), ('e', ('r',)), ('e', ('r',))): '3i',\n",
                "                   ((('e', ('r',)), ('e', ('r',))), ('r',)): 'ip',\n",
                "                   (('e', ('r', 'r')), ('e', ('r',))): 'pi',\n",
                "                   (('e', ('r',)), ('e', ('r', 'n'))): '2in',\n",
                "                   (('e', ('r',)), ('e', ('r',)), ('e', ('r', 'n'))): '3in',\n",
                "                   ((('e', ('r',)), ('e', ('r', 'n'))), ('r',)): 'inp',\n",
                "                   (('e', ('r', 'r')), ('e', ('r', 'n'))): 'pin',\n",
                "                   (('e', ('r', 'r', 'n')), ('e', ('r',))): 'pni',\n",
                "                   (('e', ('r',)), ('e', ('r',)), ('u',)): '2u-DNF',\n",
                "                   ((('e', ('r',)), ('e', ('r',)), ('u',)), ('r',)): 'up-DNF',\n",
                "                   ((('e', ('r', 'n')), ('e', ('r', 'n'))), ('n',)): '2u-DM',\n",
                "                   ((('e', ('r', 'n')), ('e', ('r', 'n'))), ('n', 'r')): 'up-DM'\n",
                "                   }\n",
                "name_query_dict = {value: key for key, value in query_name_dict.items()}\n",
                "all_tasks = list(\n",
                "    name_query_dict.keys())  # ['1p', '2p', '3p', '2i', '3i', 'ip', 'pi', '2in', '3in', 'inp', 'pin', 'pni', '2u-DNF', '2u-DM', 'up-DNF', 'up-DM']\n",
                "\n",
                "\n",
                "class Args:\n",
                "    def __init__(self, **kwargs):\n",
                "        # Basic settings\n",
                "        self.cuda = kwargs.get('cuda', True)\n",
                "        self.do_train = kwargs.get('do_train', True)\n",
                "        self.do_valid = kwargs.get('do_valid', True)\n",
                "        self.do_test = kwargs.get('do_test', True)\n",
                "\n",
                "        # Data path and dataset parameters\n",
                "        self.data_path = kwargs.get('data_path', '/path/to/data')\n",
                "        self.negative_sample_size = kwargs.get('negative_sample_size', 128)\n",
                "        self.nentity = kwargs.get('nentity', 0)  # Will be set later from stats.txt\n",
                "        self.nrelation = kwargs.get('nrelation', 0)  # Will be set later from stats.txt\n",
                "\n",
                "        # Embedding and model parameters\n",
                "        self.hidden_dim = kwargs.get('hidden_dim', 800)\n",
                "        self.gamma = kwargs.get('gamma', 60.0)  # Margin for ranking loss\n",
                "        self.geo = kwargs.get('geo', 'gamma')   # For LapE, you can set this to 'laplace' or leave as is for compatibility\n",
                "\n",
                "        # Batch and learning parameters\n",
                "        self.batch_size = kwargs.get('batch_size', 512)\n",
                "        self.test_batch_size = kwargs.get('test_batch_size', 4)\n",
                "        self.learning_rate = kwargs.get('learning_rate', 0.0001)\n",
                "        self.cpu_num = kwargs.get('cpu_num', 3)\n",
                "\n",
                "        # Checkpoint and save settings\n",
                "        self.save_path = kwargs.get('save_path', './checkpoints')\n",
                "        self.max_steps = kwargs.get('max_steps', 450001)\n",
                "        self.warm_up_steps = kwargs.get('warm_up_steps', None)\n",
                "        self.drop = kwargs.get('drop', 0.1)\n",
                "        self.save_checkpoint_steps = kwargs.get('save_checkpoint_steps', 50000)\n",
                "        self.valid_steps = kwargs.get('valid_steps', 30000)\n",
                "        self.log_steps = kwargs.get('log_steps', 100)\n",
                "        self.test_log_steps = kwargs.get('test_log_steps', 1000)\n",
                "\n",
                "        # Query and task settings\n",
                "        self.tasks = kwargs.get('tasks', '1p.2p.3p.2i.3i.ip.pi.2in.3in.inp.pin.pni.2u.up')\n",
                "        self.evaluate_union = kwargs.get('evaluate_union', \"DNF\")\n",
                "        self.print_on_screen = kwargs.get('print_on_screen', True)\n",
                "        \n",
                "        # Seed and mode settings\n",
                "        self.seed = kwargs.get('seed', 42)\n",
                "        self.beta_mode = kwargs.get('beta_mode', \"(1600,2)\")\n",
                "        self.gamma_mode = kwargs.get('gamma_mode', \"(1600,4)\")\n",
                "        self.box_mode = kwargs.get('box_mode', \"(none,0.02)\")\n",
                "        self.prefix = kwargs.get('prefix', None)\n",
                "        self.checkpoint_path = kwargs.get('checkpoint_path', None)\n",
                "        \n",
                "def save_model(model, optimizer, save_variable_list, args):\n",
                "    \"\"\"\n",
                "    Save the parameters of the model and the optimizer,\n",
                "    as well as some other variables such as step and learning_rate.\n",
                "    (Works for LapE model)\n",
                "    \"\"\"\n",
                "    argparse_dict = vars(args)\n",
                "    with open(os.path.join(args.save_path, 'config.json'), 'w') as fjson:\n",
                "        json.dump(argparse_dict, fjson)\n",
                "\n",
                "    torch.save({\n",
                "        **save_variable_list,\n",
                "        'model_state_dict': model.state_dict(),\n",
                "        'optimizer_state_dict': optimizer.state_dict()\n",
                "    }, os.path.join(args.save_path, 'checkpoint'))\n",
                "\n",
                "\n",
                "def set_logger(args):\n",
                "    \"\"\"\n",
                "    Configure logging to output to both console and a log file.\n",
                "    (Works for LapE model)\n",
                "    \"\"\"\n",
                "    if args.do_train:\n",
                "        log_file = os.path.join(args.save_path, 'train.log')\n",
                "    else:\n",
                "        log_file = os.path.join(args.save_path, 'test.log')\n",
                "\n",
                "    logging.basicConfig(\n",
                "        format='%(asctime)s %(levelname)-8s %(message)s',\n",
                "        level=logging.INFO,\n",
                "        datefmt='%Y-%m-%d %H:%M:%S',\n",
                "        filename=log_file,\n",
                "        filemode='a+'\n",
                "    )\n",
                "    if args.print_on_screen:\n",
                "        console = logging.StreamHandler()\n",
                "        console.setLevel(logging.INFO)\n",
                "        formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
                "        console.setFormatter(formatter)\n",
                "        logging.getLogger('').addHandler(console)\n",
                "def log_metrics(mode, step, metrics):\n",
                "    \"\"\"\n",
                "    Print the evaluation logs.\n",
                "    (This function is generic and works for LapE as well.)\n",
                "    \"\"\"\n",
                "    for metric in metrics:\n",
                "        logging.info('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n",
                "def evaluate(model, tp_answers, fn_answers, args, dataloader, query_name_dict, mode, step, writer):\n",
                "    '''\n",
                "    Evaluate queries in the dataloader for the LapE model.\n",
                "    This function computes metrics (MRR, Hits@K, etc.) by calling the model's test_step,\n",
                "    and then logs and aggregates the results.\n",
                "    '''\n",
                "    average_metrics = defaultdict(float)\n",
                "    all_metrics = defaultdict(float)\n",
                "\n",
                "    metrics = model.test_step(model, tp_answers, fn_answers, args, dataloader, query_name_dict)\n",
                "    num_query_structures = 0\n",
                "    num_queries = 0\n",
                "    for query_structure in metrics:\n",
                "        log_metrics(mode + \" \" + query_name_dict[query_structure], step, metrics[query_structure])\n",
                "        for metric in metrics[query_structure]:\n",
                "            writer.add_scalar(\"_\".join([mode, query_name_dict[query_structure], metric]),\n",
                "                              metrics[query_structure][metric], step)\n",
                "            all_metrics[\"_\".join([query_name_dict[query_structure], metric])] = metrics[query_structure][metric]\n",
                "            if metric != 'num_queries':\n",
                "                average_metrics[metric] += metrics[query_structure][metric]\n",
                "        num_queries += metrics[query_structure]['num_queries']\n",
                "        num_query_structures += 1\n",
                "\n",
                "    for metric in average_metrics:\n",
                "        average_metrics[metric] /= num_query_structures\n",
                "        writer.add_scalar(\"_\".join([mode, 'average', metric]), average_metrics[metric], step)\n",
                "        all_metrics[\"_\".join([\"average\", metric])] = average_metrics[metric]\n",
                "    log_metrics('%s average' % mode, step, average_metrics)\n",
                "\n",
                "    return all_metrics\n",
                "\n",
                "def load_data(args, tasks):\n",
                "    '''\n",
                "    Load queries and remove queries not in tasks.\n",
                "    This function is independent of the embedding distribution (Gamma, Laplace, etc.)\n",
                "    and can be used as-is for the LapE model.\n",
                "    '''\n",
                "    logging.info(\"loading data\")\n",
                "    train_queries = pickle.load(open(os.path.join(args.data_path, \"train-queries.pkl\"), 'rb'))\n",
                "    train_answers = pickle.load(open(os.path.join(args.data_path, \"train-answers.pkl\"), 'rb'))\n",
                "    valid_queries = pickle.load(open(os.path.join(args.data_path, \"valid-queries.pkl\"), 'rb'))\n",
                "    valid_hard_answers = pickle.load(open(os.path.join(args.data_path, \"valid-hard-answers.pkl\"), 'rb'))\n",
                "    valid_easy_answers = pickle.load(open(os.path.join(args.data_path, \"valid-easy-answers.pkl\"), 'rb'))\n",
                "    test_queries = pickle.load(open(os.path.join(args.data_path, \"test-queries.pkl\"), 'rb'))\n",
                "    test_hard_answers = pickle.load(open(os.path.join(args.data_path, \"test-hard-answers.pkl\"), 'rb'))\n",
                "    test_easy_answers = pickle.load(open(os.path.join(args.data_path, \"test-easy-answers.pkl\"), 'rb'))\n",
                "\n",
                "    # remove tasks not in args.tasks\n",
                "    for name in all_tasks:\n",
                "        if 'u' in name:\n",
                "            name, evaluate_union = name.split('-')\n",
                "        else:\n",
                "            evaluate_union = args.evaluate_union\n",
                "        if name not in tasks or evaluate_union != args.evaluate_union:\n",
                "            query_structure = name_query_dict[name if 'u' not in name else '-'.join([name, evaluate_union])]\n",
                "            if query_structure in train_queries:\n",
                "                del train_queries[query_structure]\n",
                "            if query_structure in valid_queries:\n",
                "                del valid_queries[query_structure]\n",
                "            if query_structure in test_queries:\n",
                "                del test_queries[query_structure]\n",
                "\n",
                "    return train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, test_queries, test_hard_answers, test_easy_answers\n",
                "\n",
                "def main(args):\n",
                "    set_global_seed(args.seed)\n",
                "    tasks = args.tasks.split('.')\n",
                "    for task in tasks:\n",
                "        if 'n' in task and args.geo in ['box', 'vec']:\n",
                "            assert False, \"Q2B and GQE cannot handle queries with negation\"\n",
                "    # For union evaluation, our LapE model uses the same scheme as GammaE\n",
                "    if args.evaluate_union == 'DM':\n",
                "        assert args.geo == 'gamma', \"only BetaE supports modeling union using De Morgan's Laws\"\n",
                "\n",
                "    cur_time = parse_time()\n",
                "    prefix = args.prefix if args.prefix is not None else 'logs'\n",
                "\n",
                "    print(\"overwriting args.save_path\")\n",
                "    args.save_path = os.path.join(prefix, os.path.basename(args.data_path), args.tasks, args.geo)\n",
                "    if args.geo in ['box']:\n",
                "        tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.box_mode)\n",
                "    elif args.geo in ['vec']:\n",
                "        tmp_str = \"g-{}\".format(args.gamma)\n",
                "    elif args.geo == 'beta':\n",
                "        tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.beta_mode)\n",
                "    elif args.geo == 'gamma':\n",
                "        tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.gamma_mode)\n",
                "    # For our LapE model, we typically set args.geo to 'laplace'\n",
                "    else: tmp_str = \"g-{}-mode-{}\".format(args.gamma, args.geo)  # Uses args.geo directly\n",
                "\n",
                "    if args.checkpoint_path is not None:\n",
                "        args.save_path = args.checkpoint_path\n",
                "    else:\n",
                "        args.save_path = os.path.join(args.save_path, tmp_str, cur_time)\n",
                "\n",
                "    if not os.path.exists(args.save_path):\n",
                "        os.makedirs(args.save_path)\n",
                "\n",
                "    print(\"logging to\", args.save_path)\n",
                "    writer = SummaryWriter(args.save_path) if args.do_train else SummaryWriter('./logs-debug/unused-tb')\n",
                "    set_logger(args)\n",
                "\n",
                "    with open(os.path.join(args.data_path, 'stats.txt')) as f:\n",
                "        entrel = f.readlines()\n",
                "        nentity = int(entrel[0].split(' ')[-1])\n",
                "        nrelation = int(entrel[1].split(' ')[-1])\n",
                "    args.nentity = nentity\n",
                "    args.nrelation = nrelation\n",
                "\n",
                "    logging.info('-------------------------------' * 3)\n",
                "    logging.info('Geo: %s' % args.geo)\n",
                "    logging.info('seed: %d' % args.seed)\n",
                "    logging.info('Data Path: %s' % args.data_path)\n",
                "    logging.info('#entity: %d' % nentity)\n",
                "    logging.info('#relation: %d' % nrelation)\n",
                "    logging.info('#max steps: %d' % args.max_steps)\n",
                "    logging.info('Evaluate unions using: %s' % args.evaluate_union)\n",
                "\n",
                "    train_queries, train_answers, valid_queries, valid_hard_answers, valid_easy_answers, \\\n",
                "        test_queries, test_hard_answers, test_easy_answers = load_data(args, tasks)\n",
                "    # Merge union queries from valid into train queries\n",
                "    train_queries[(('e', ('r',)), ('e', ('r',)), ('u',))].update(valid_queries[(('e', ('r',)), ('e', ('r',)), ('u',))])\n",
                "    train_queries[((('e', ('r',)), ('e', ('r',)), ('u',)), ('r',))].update(\n",
                "        valid_queries[((('e', ('r',)), ('e', ('r',)), ('u',)), ('r',))])\n",
                "    # for key in valid_easy_answers.keys():\n",
                "    #     valid_easy_answers[key] = valid_easy_answers[key].union(valid_hard_answers[key])\n",
                "    for key, valid_set in valid_hard_answers.items():\n",
                "        if key in train_answers:\n",
                "            train_answers[key] = train_answers[key].union(valid_set)\n",
                "        else:\n",
                "            train_answers[key] = valid_set\n",
                "    logging.info(\"Training info:\")\n",
                "    if args.do_train:\n",
                "        for query_structure in train_queries:\n",
                "            logging.info(query_name_dict[query_structure] + \": \" + str(len(train_queries[query_structure])))\n",
                "        train_path_queries = defaultdict(set)\n",
                "        train_other_queries = defaultdict(set)\n",
                "        path_list = ['1p', '2p', '3p']\n",
                "        for query_structure in train_queries:\n",
                "            if query_name_dict[query_structure] in path_list:\n",
                "                train_path_queries[query_structure] = train_queries[query_structure]\n",
                "            else:\n",
                "                train_other_queries[query_structure] = train_queries[query_structure]\n",
                "        train_path_queries = flatten_query(train_path_queries)\n",
                "        train_path_iterator = SingledirectionalOneShotIterator(DataLoader(\n",
                "            TrainDataset(train_path_queries, nentity, nrelation, args.negative_sample_size, train_answers),\n",
                "            batch_size=args.batch_size,\n",
                "            shuffle=True,\n",
                "            num_workers=args.cpu_num,\n",
                "            collate_fn=TrainDataset.collate_fn\n",
                "        ))\n",
                "        if len(train_other_queries) > 0:\n",
                "            train_other_queries = flatten_query(train_other_queries)\n",
                "            train_other_iterator = SingledirectionalOneShotIterator(DataLoader(\n",
                "                TrainDataset(train_other_queries, nentity, nrelation, args.negative_sample_size, train_answers),\n",
                "                batch_size=args.batch_size,\n",
                "                shuffle=True,\n",
                "                num_workers=args.cpu_num,\n",
                "                collate_fn=TrainDataset.collate_fn\n",
                "            ))\n",
                "        else:\n",
                "            train_other_iterator = None\n",
                "\n",
                "    logging.info(\"Validation info:\")\n",
                "    if args.do_valid:\n",
                "        for query_structure in valid_queries:\n",
                "            logging.info(query_name_dict[query_structure] + \": \" + str(len(valid_queries[query_structure])))\n",
                "        valid_queries = flatten_query(valid_queries)\n",
                "        valid_dataloader = DataLoader(\n",
                "            TestDataset(valid_queries, args.nentity, args.nrelation),\n",
                "            batch_size=args.test_batch_size,\n",
                "            num_workers=args.cpu_num,\n",
                "            collate_fn=TestDataset.collate_fn\n",
                "        )\n",
                "\n",
                "    logging.info(\"Test info:\")\n",
                "    if args.do_test:\n",
                "        for query_structure in test_queries:\n",
                "            logging.info(query_name_dict[query_structure] + \": \" + str(len(test_queries[query_structure])))\n",
                "        test_queries = flatten_query(test_queries)\n",
                "        test_dataloader = DataLoader(\n",
                "            TestDataset(test_queries, args.nentity, args.nrelation),\n",
                "            batch_size=args.test_batch_size,\n",
                "            num_workers=args.cpu_num,\n",
                "            collate_fn=TestDataset.collate_fn\n",
                "        )\n",
                "\n",
                "    # Initialize our LapE model (KGReasoningLapE) instead of GammaE\n",
                "    model = KGReasoningLapE(\n",
                "        nentity=nentity,\n",
                "        nrelation=nrelation,\n",
                "        hidden_dim=args.hidden_dim,\n",
                "        gamma=args.gamma,\n",
                "        geo=args.geo,\n",
                "        use_cuda=args.cuda,\n",
                "        box_mode=eval_tuple(args.box_mode),\n",
                "        beta_mode=eval_tuple(args.beta_mode),\n",
                "        gamma_mode=eval_tuple(args.gamma_mode),\n",
                "        test_batch_size=args.test_batch_size,\n",
                "        query_name_dict=query_name_dict,\n",
                "        drop=args.drop\n",
                "    )\n",
                "\n",
                "    logging.info('Model Parameter Configuration:')\n",
                "    num_params = 0\n",
                "    for name, param in model.named_parameters():\n",
                "        logging.info('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
                "        if param.requires_grad:\n",
                "            num_params += np.prod(param.size())\n",
                "    logging.info('Parameter Number: %d' % num_params)\n",
                "\n",
                "    if args.cuda:\n",
                "        model = model.cuda()\n",
                "\n",
                "    if args.do_train:\n",
                "        current_learning_rate = args.learning_rate\n",
                "        optimizer = torch.optim.Adam(\n",
                "            filter(lambda p: p.requires_grad, model.parameters()),\n",
                "            lr=current_learning_rate\n",
                "        )\n",
                "        warm_up_steps = args.max_steps // 2\n",
                "\n",
                "    if args.checkpoint_path is not None:\n",
                "        logging.info('Loading checkpoint %s...' % args.checkpoint_path)\n",
                "        checkpoint = torch.load(os.path.join(args.checkpoint_path, 'checkpoint'))\n",
                "        init_step = checkpoint['step']\n",
                "        model.load_state_dict(checkpoint['model_state_dict'])\n",
                "        if args.do_train:\n",
                "            current_learning_rate = checkpoint['current_learning_rate']\n",
                "            warm_up_steps = checkpoint['warm_up_steps']\n",
                "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
                "    else:\n",
                "        logging.info('Randomly Initializing %s Model...' % args.geo)\n",
                "        init_step = 0\n",
                "\n",
                "    step = init_step\n",
                "    # For LapE, we can log the mode as 'laplace'\n",
                "    if args.geo == 'box':\n",
                "        logging.info('box mode = %s' % args.box_mode)\n",
                "    elif args.geo == 'beta':\n",
                "        logging.info('beta mode = %s' % args.beta_mode)\n",
                "    elif args.geo == 'gamma':\n",
                "        logging.info('gamma mode = %s' % args.gamma_mode)\n",
                "    else:\n",
                "        logging.info('Using Laplace-based embeddings for KG reasoning.')\n",
                "    logging.info('tasks = %s' % args.tasks)\n",
                "    logging.info('init_step = %d' % init_step)\n",
                "    if args.do_train:\n",
                "        logging.info('Start Training...')\n",
                "        logging.info('learning_rate = %f' % current_learning_rate)\n",
                "    logging.info('batch_size = %d' % args.batch_size)\n",
                "    logging.info('hidden_dim = %d' % args.hidden_dim)\n",
                "    logging.info('gamma = %f' % args.gamma)\n",
                "\n",
                "    if args.do_train:\n",
                "        training_logs = []\n",
                "        # Training Loop\n",
                "        for step in range(init_step, args.max_steps):\n",
                "            if step == 2 * args.max_steps // 3:\n",
                "                args.valid_steps *= 4\n",
                "\n",
                "            log = model.train_step(model, optimizer, train_path_iterator, args, step)\n",
                "            for metric in log:\n",
                "                writer.add_scalar('path_' + metric, log[metric], step)\n",
                "            if train_other_iterator is not None:\n",
                "                log = model.train_step(model, optimizer, train_other_iterator, args, step)\n",
                "                for metric in log:\n",
                "                    writer.add_scalar('other_' + metric, log[metric], step)\n",
                "                log = model.train_step(model, optimizer, train_path_iterator, args, step)\n",
                "\n",
                "            training_logs.append(log)\n",
                "\n",
                "            if step >= warm_up_steps:\n",
                "                current_learning_rate = current_learning_rate / 5\n",
                "                logging.info('Change learning_rate to %f at step %d' % (current_learning_rate, step))\n",
                "                optimizer = torch.optim.Adam(\n",
                "                    filter(lambda p: p.requires_grad, model.parameters()),\n",
                "                    lr=current_learning_rate\n",
                "                )\n",
                "                warm_up_steps = warm_up_steps * 1.5\n",
                "\n",
                "            if step % args.save_checkpoint_steps == 0:\n",
                "                save_variable_list = {\n",
                "                    'step': step,\n",
                "                    'current_learning_rate': current_learning_rate,\n",
                "                    'warm_up_steps': warm_up_steps\n",
                "                }\n",
                "                save_model(model, optimizer, save_variable_list, args)\n",
                "\n",
                "            if step % args.valid_steps == 0 and step > 0:\n",
                "                if args.do_valid:\n",
                "                    logging.info('Evaluating on Valid Dataset...')\n",
                "                    valid_all_metrics = evaluate(model, valid_easy_answers, valid_hard_answers, args, valid_dataloader,\n",
                "                                                 query_name_dict, 'Valid', step, writer)\n",
                "                if args.do_test:\n",
                "                    logging.info('Evaluating on Test Dataset...')\n",
                "                    test_all_metrics = evaluate(model, test_easy_answers, test_hard_answers, args, test_dataloader,\n",
                "                                                query_name_dict, 'Test', step, writer)\n",
                "\n",
                "            if step % args.log_steps == 0:\n",
                "                metrics = {}\n",
                "                for metric in training_logs[0].keys():\n",
                "                    metrics[metric] = sum([log[metric] for log in training_logs]) / len(training_logs)\n",
                "                log_metrics('Training average', step, metrics)\n",
                "                training_logs = []\n",
                "\n",
                "        save_variable_list = {\n",
                "            'step': step,\n",
                "            'current_learning_rate': current_learning_rate,\n",
                "            'warm_up_steps': warm_up_steps\n",
                "        }\n",
                "        save_model(model, optimizer, save_variable_list, args)\n",
                "\n",
                "    try:\n",
                "        print(step)\n",
                "    except:\n",
                "        step = 0\n",
                "\n",
                "    if args.do_test:\n",
                "        logging.info('Evaluating on Test Dataset...')\n",
                "        test_all_metrics = evaluate(model, test_easy_answers, test_hard_answers, args, test_dataloader, query_name_dict,\n",
                "                                    'Test', step, writer)\n",
                "\n",
                "    logging.info(\"Training finished!!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Calculation for converting training step and number of epochs\n",
                "we use training step when we want to control training based on the training time and not on the number of data points seen\n",
                "To calculate the number of **epochs**, we need:  \n",
                "1. **Dataset size** (number of training triples in FB15k-237).  \n",
                "2. **Batch size** (given as `-b 512`).  \n",
                "3. **Total steps** (`--max_steps 450001`).  \n",
                "\n",
                "### **Step 1: Get Dataset Size**  \n",
                "The **FB15k-237** dataset has **272,115 training triples**.\n",
                "\n",
                "### **Step 2: Compute Steps per Epoch**  \n",
                "Each epoch means processing **all** training triples once.  \n",
                "$$\n",
                "\\text{Steps per epoch} = \\frac{\\text{Training triples}}{\\text{Batch size}} = \\frac{272115}{512} \\approx 532 \\text{ steps per epoch}\n",
                "$$\n",
                "\n",
                "### **Step 3: Compute Total Epochs**  \n",
                "Total epochs =  \n",
                "$$\n",
                "\\frac{\\text{max\\_steps}}{\\text{steps per epoch}} = \\frac{450001}{532} \\approx 846 \\text{ epochs}\n",
                "$$\n",
                "\n",
                "### **Final Answer:**  \n",
                "With `max_steps = 450001`, the model will train for **~846 epochs** on FB15k-237.  \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "execution_failed": "2025-03-20T15:47:47.495Z",
                    "iopub.execute_input": "2025-03-20T15:27:47.423053Z",
                    "iopub.status.busy": "2025-03-20T15:27:47.422777Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-03-20 15:27:47,470 INFO     ---------------------------------------------------------------------------------------------\n",
                        "2025-03-20 15:27:47,470 INFO     Geo: laplace\n",
                        "2025-03-20 15:27:47,472 INFO     seed: 42\n",
                        "2025-03-20 15:27:47,472 INFO     Data Path: /kaggle/input/kg-data/FB15k-237-betae\n",
                        "2025-03-20 15:27:47,473 INFO     #entity: 14505\n",
                        "2025-03-20 15:27:47,474 INFO     #relation: 474\n",
                        "2025-03-20 15:27:47,474 INFO     #max steps: 1000\n",
                        "2025-03-20 15:27:47,475 INFO     Evaluate unions using: DNF\n",
                        "2025-03-20 15:27:47,476 INFO     loading data\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "overwriting args.save_path\n",
                        "logging to logs/FB15k-237-betae/1p.2p.3p.2i.3i.ip.pi.2in.3in.inp.pin.pni.2u.up/laplace/g-60.0-mode-laplace/2025.03.20-15:27:47\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-03-20 15:28:23,604 INFO     Training info:\n",
                        "2025-03-20 15:28:23,605 INFO     1p: 149689\n",
                        "2025-03-20 15:28:23,605 INFO     2p: 149689\n",
                        "2025-03-20 15:28:23,606 INFO     3p: 149689\n",
                        "2025-03-20 15:28:23,607 INFO     2i: 149689\n",
                        "2025-03-20 15:28:23,608 INFO     3i: 149689\n",
                        "2025-03-20 15:28:23,609 INFO     2in: 14968\n",
                        "2025-03-20 15:28:23,610 INFO     3in: 14968\n",
                        "2025-03-20 15:28:23,611 INFO     inp: 14968\n",
                        "2025-03-20 15:28:23,612 INFO     pin: 14968\n",
                        "2025-03-20 15:28:23,612 INFO     pni: 14968\n",
                        "2025-03-20 15:28:23,613 INFO     2u-DNF: 5000\n",
                        "2025-03-20 15:28:23,613 INFO     up-DNF: 5000\n",
                        "2025-03-20 15:28:25,110 INFO     Validation info:\n",
                        "2025-03-20 15:28:25,111 INFO     1p: 20094\n",
                        "2025-03-20 15:28:25,111 INFO     2p: 5000\n",
                        "2025-03-20 15:28:25,112 INFO     3p: 5000\n",
                        "2025-03-20 15:28:25,113 INFO     2i: 5000\n",
                        "2025-03-20 15:28:25,113 INFO     3i: 5000\n",
                        "2025-03-20 15:28:25,114 INFO     ip: 5000\n",
                        "2025-03-20 15:28:25,115 INFO     pi: 5000\n",
                        "2025-03-20 15:28:25,115 INFO     2in: 5000\n",
                        "2025-03-20 15:28:25,116 INFO     3in: 5000\n",
                        "2025-03-20 15:28:25,116 INFO     inp: 5000\n",
                        "2025-03-20 15:28:25,117 INFO     pin: 5000\n",
                        "2025-03-20 15:28:25,118 INFO     pni: 5000\n",
                        "2025-03-20 15:28:25,118 INFO     2u-DNF: 5000\n",
                        "2025-03-20 15:28:25,119 INFO     up-DNF: 5000\n",
                        "2025-03-20 15:28:25,147 INFO     Test info:\n",
                        "2025-03-20 15:28:25,147 INFO     1p: 22804\n",
                        "2025-03-20 15:28:25,148 INFO     2p: 5000\n",
                        "2025-03-20 15:28:25,148 INFO     3p: 5000\n",
                        "2025-03-20 15:28:25,149 INFO     2i: 5000\n",
                        "2025-03-20 15:28:25,150 INFO     3i: 5000\n",
                        "2025-03-20 15:28:25,151 INFO     ip: 5000\n",
                        "2025-03-20 15:28:25,151 INFO     pi: 5000\n",
                        "2025-03-20 15:28:25,152 INFO     2in: 5000\n",
                        "2025-03-20 15:28:25,153 INFO     3in: 5000\n",
                        "2025-03-20 15:28:25,156 INFO     inp: 5000\n",
                        "2025-03-20 15:28:25,156 INFO     pin: 5000\n",
                        "2025-03-20 15:28:25,157 INFO     pni: 5000\n",
                        "2025-03-20 15:28:25,158 INFO     2u-DNF: 5000\n",
                        "2025-03-20 15:28:25,159 INFO     up-DNF: 5000\n",
                        "2025-03-20 15:28:26,066 INFO     Model Parameter Configuration:\n",
                        "2025-03-20 15:28:26,067 INFO     Parameter gamma: torch.Size([1]), require_grad = False\n",
                        "2025-03-20 15:28:26,068 INFO     Parameter embedding_range: torch.Size([1]), require_grad = False\n",
                        "2025-03-20 15:28:26,069 INFO     Parameter entity_embedding: torch.Size([14505, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,071 INFO     Parameter relation_embedding: torch.Size([474, 800]), require_grad = True\n",
                        "2025-03-20 15:28:26,072 INFO     Parameter mu_relation: torch.Size([474, 800]), require_grad = True\n",
                        "2025-03-20 15:28:26,073 INFO     Parameter b_relation: torch.Size([474, 800]), require_grad = True\n",
                        "2025-03-20 15:28:26,074 INFO     Parameter modulus: torch.Size([1]), require_grad = True\n",
                        "2025-03-20 15:28:26,075 INFO     Parameter center_net.layer_mu1.weight: torch.Size([800, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,075 INFO     Parameter center_net.layer_mu1.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,076 INFO     Parameter center_net.layer_mu2.weight: torch.Size([800, 800]), require_grad = True\n",
                        "2025-03-20 15:28:26,077 INFO     Parameter center_net.layer_mu2.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,079 INFO     Parameter center_net.layer_b1.weight: torch.Size([800, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,080 INFO     Parameter center_net.layer_b1.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,080 INFO     Parameter center_net.layer_b2.weight: torch.Size([800, 800]), require_grad = True\n",
                        "2025-03-20 15:28:26,081 INFO     Parameter center_net.layer_b2.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,082 INFO     Parameter projection_net.layer_mu1.weight: torch.Size([1600, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,082 INFO     Parameter projection_net.layer_mu1.bias: torch.Size([1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,083 INFO     Parameter projection_net.layer_mu0.weight: torch.Size([800, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,084 INFO     Parameter projection_net.layer_mu0.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,085 INFO     Parameter projection_net.layer_b1.weight: torch.Size([1600, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,086 INFO     Parameter projection_net.layer_b1.bias: torch.Size([1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,086 INFO     Parameter projection_net.layer_b0.weight: torch.Size([800, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,087 INFO     Parameter projection_net.layer_b0.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,088 INFO     Parameter projection_net.layer_mu2.weight: torch.Size([1600, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,088 INFO     Parameter projection_net.layer_mu2.bias: torch.Size([1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,090 INFO     Parameter projection_net.layer_b2.weight: torch.Size([1600, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,090 INFO     Parameter projection_net.layer_b2.bias: torch.Size([1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,091 INFO     Parameter projection_net.layer_mu3.weight: torch.Size([1600, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,092 INFO     Parameter projection_net.layer_mu3.bias: torch.Size([1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,095 INFO     Parameter projection_net.layer_b3.weight: torch.Size([1600, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,095 INFO     Parameter projection_net.layer_b3.bias: torch.Size([1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,097 INFO     Parameter projection_net.layer_mu4.weight: torch.Size([1600, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,098 INFO     Parameter projection_net.layer_mu4.bias: torch.Size([1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,098 INFO     Parameter projection_net.layer_b4.weight: torch.Size([1600, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,099 INFO     Parameter projection_net.layer_b4.bias: torch.Size([1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,100 INFO     Parameter union_net.layer_mu1.weight: torch.Size([800, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,100 INFO     Parameter union_net.layer_mu1.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,101 INFO     Parameter union_net.layer_mu2.weight: torch.Size([400, 800]), require_grad = True\n",
                        "2025-03-20 15:28:26,103 INFO     Parameter union_net.layer_mu2.bias: torch.Size([400]), require_grad = True\n",
                        "2025-03-20 15:28:26,104 INFO     Parameter union_net.layer_mu3.weight: torch.Size([800, 400]), require_grad = True\n",
                        "2025-03-20 15:28:26,105 INFO     Parameter union_net.layer_mu3.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,106 INFO     Parameter union_net.layer_b1.weight: torch.Size([800, 1600]), require_grad = True\n",
                        "2025-03-20 15:28:26,106 INFO     Parameter union_net.layer_b1.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,107 INFO     Parameter union_net.layer_b2.weight: torch.Size([400, 800]), require_grad = True\n",
                        "2025-03-20 15:28:26,109 INFO     Parameter union_net.layer_b2.bias: torch.Size([400]), require_grad = True\n",
                        "2025-03-20 15:28:26,110 INFO     Parameter union_net.layer_b3.weight: torch.Size([800, 400]), require_grad = True\n",
                        "2025-03-20 15:28:26,111 INFO     Parameter union_net.layer_b3.bias: torch.Size([800]), require_grad = True\n",
                        "2025-03-20 15:28:26,112 INFO     Parameter Number: 55087201\n",
                        "2025-03-20 15:28:28,063 INFO     Randomly Initializing laplace Model...\n",
                        "2025-03-20 15:28:28,064 INFO     Using Laplace-based embeddings for KG reasoning.\n",
                        "2025-03-20 15:28:28,065 INFO     tasks = 1p.2p.3p.2i.3i.ip.pi.2in.3in.inp.pin.pni.2u.up\n",
                        "2025-03-20 15:28:28,065 INFO     init_step = 0\n",
                        "2025-03-20 15:28:28,066 INFO     Start Training...\n",
                        "2025-03-20 15:28:28,067 INFO     learning_rate = 0.000100\n",
                        "2025-03-20 15:28:28,067 INFO     batch_size = 512\n",
                        "2025-03-20 15:28:28,068 INFO     hidden_dim = 800\n",
                        "2025-03-20 15:28:28,069 INFO     gamma = 60.000000\n",
                        "2025-03-20 15:28:31,594 INFO     Training average positive_sample_loss at step 0: 128.654922\n",
                        "2025-03-20 15:28:31,596 INFO     Training average negative_sample_loss at step 0: 0.000000\n",
                        "2025-03-20 15:28:31,597 INFO     Training average loss at step 0: 64.327461\n",
                        "2025-03-20 15:29:19,575 INFO     Training average positive_sample_loss at step 100: 106.434143\n",
                        "2025-03-20 15:29:19,576 INFO     Training average negative_sample_loss at step 100: 0.000013\n",
                        "2025-03-20 15:29:19,576 INFO     Training average loss at step 100: 53.217078\n",
                        "2025-03-20 15:30:09,713 INFO     Training average positive_sample_loss at step 200: 87.529747\n",
                        "2025-03-20 15:30:09,714 INFO     Training average negative_sample_loss at step 200: 0.000031\n",
                        "2025-03-20 15:30:09,715 INFO     Training average loss at step 200: 43.764889\n",
                        "2025-03-20 15:30:59,990 INFO     Training average positive_sample_loss at step 300: 75.612403\n",
                        "2025-03-20 15:30:59,990 INFO     Training average negative_sample_loss at step 300: 0.000045\n",
                        "2025-03-20 15:30:59,991 INFO     Training average loss at step 300: 37.806224\n",
                        "2025-03-20 15:31:50,368 INFO     Training average positive_sample_loss at step 400: 65.405183\n",
                        "2025-03-20 15:31:50,368 INFO     Training average negative_sample_loss at step 400: 0.000050\n",
                        "2025-03-20 15:31:50,369 INFO     Training average loss at step 400: 32.702617\n",
                        "2025-03-20 15:32:41,640 INFO     Change learning_rate to 0.000020 at step 500\n",
                        "2025-03-20 15:32:41,644 INFO     Evaluating on Valid Dataset...\n",
                        "  0%|          | 0/21274 [00:00<?, ?it/s]2025-03-20 15:32:42,160 INFO     Evaluating the model... (0/21274)\n",
                        "  5%|▍         | 996/21274 [00:24<08:03, 41.98it/s]2025-03-20 15:33:06,184 INFO     Evaluating the model... (1000/21274)\n",
                        "  9%|▉         | 1996/21274 [00:48<07:39, 41.95it/s]2025-03-20 15:33:30,098 INFO     Evaluating the model... (2000/21274)\n",
                        " 14%|█▍        | 2996/21274 [01:12<07:19, 41.56it/s]2025-03-20 15:33:54,099 INFO     Evaluating the model... (3000/21274)\n",
                        " 19%|█▉        | 3996/21274 [01:36<06:51, 42.02it/s]2025-03-20 15:34:18,033 INFO     Evaluating the model... (4000/21274)\n",
                        " 23%|██▎       | 4996/21274 [02:00<06:28, 41.87it/s]2025-03-20 15:34:41,953 INFO     Evaluating the model... (5000/21274)\n",
                        " 28%|██▊       | 5997/21274 [02:25<06:24, 39.73it/s]2025-03-20 15:35:06,848 INFO     Evaluating the model... (6000/21274)\n",
                        " 33%|███▎      | 7000/21274 [02:51<06:12, 38.29it/s]2025-03-20 15:35:32,695 INFO     Evaluating the model... (7000/21274)\n",
                        " 38%|███▊      | 7998/21274 [03:16<05:41, 38.92it/s]2025-03-20 15:35:58,544 INFO     Evaluating the model... (8000/21274)\n",
                        " 42%|████▏     | 9000/21274 [03:42<05:23, 37.98it/s]2025-03-20 15:36:24,294 INFO     Evaluating the model... (9000/21274)\n",
                        " 47%|████▋     | 10000/21274 [04:08<04:49, 38.95it/s]2025-03-20 15:36:50,518 INFO     Evaluating the model... (10000/21274)\n",
                        " 52%|█████▏    | 11000/21274 [04:35<04:32, 37.65it/s]2025-03-20 15:37:16,829 INFO     Evaluating the model... (11000/21274)\n",
                        " 56%|█████▋    | 12000/21274 [05:01<04:02, 38.31it/s]2025-03-20 15:37:43,176 INFO     Evaluating the model... (12000/21274)\n",
                        " 61%|██████    | 13000/21274 [05:27<03:35, 38.37it/s]2025-03-20 15:38:09,313 INFO     Evaluating the model... (13000/21274)\n",
                        " 66%|██████▌   | 13997/21274 [05:53<03:12, 37.71it/s]2025-03-20 15:38:35,221 INFO     Evaluating the model... (14000/21274)\n",
                        " 70%|███████   | 14997/21274 [06:19<02:48, 37.23it/s]2025-03-20 15:39:01,328 INFO     Evaluating the model... (15000/21274)\n",
                        " 75%|███████▌  | 15997/21274 [06:45<02:17, 38.48it/s]2025-03-20 15:39:27,525 INFO     Evaluating the model... (16000/21274)\n",
                        " 80%|███████▉  | 16997/21274 [07:11<01:53, 37.68it/s]2025-03-20 15:39:53,592 INFO     Evaluating the model... (17000/21274)\n",
                        " 85%|████████▍ | 17997/21274 [07:37<01:25, 38.32it/s]2025-03-20 15:40:19,640 INFO     Evaluating the model... (18000/21274)\n",
                        " 89%|████████▉ | 18997/21274 [08:03<00:58, 39.05it/s]2025-03-20 15:40:45,614 INFO     Evaluating the model... (19000/21274)\n",
                        " 94%|█████████▍| 19999/21274 [08:29<00:32, 39.52it/s]2025-03-20 15:41:11,115 INFO     Evaluating the model... (20000/21274)\n",
                        " 99%|█████████▊| 20999/21274 [08:55<00:07, 37.71it/s]2025-03-20 15:41:37,481 INFO     Evaluating the model... (21000/21274)\n",
                        "100%|██████████| 21274/21274 [09:03<00:00, 39.15it/s]\n",
                        "2025-03-20 15:41:45,046 INFO     Valid 1p MRR at step 500: 0.181564\n",
                        "2025-03-20 15:41:45,046 INFO     Valid 1p HITS1 at step 500: 0.154227\n",
                        "2025-03-20 15:41:45,047 INFO     Valid 1p HITS3 at step 500: 0.199890\n",
                        "2025-03-20 15:41:45,048 INFO     Valid 1p HITS10 at step 500: 0.222490\n",
                        "2025-03-20 15:41:45,048 INFO     Valid 1p num_queries at step 500: 20094.000000\n",
                        "2025-03-20 15:41:45,051 INFO     Valid 2p MRR at step 500: 0.005534\n",
                        "2025-03-20 15:41:45,053 INFO     Valid 2p HITS1 at step 500: 0.003726\n",
                        "2025-03-20 15:41:45,054 INFO     Valid 2p HITS3 at step 500: 0.005332\n",
                        "2025-03-20 15:41:45,054 INFO     Valid 2p HITS10 at step 500: 0.007234\n",
                        "2025-03-20 15:41:45,055 INFO     Valid 2p num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,056 INFO     Valid 3p MRR at step 500: 0.004392\n",
                        "2025-03-20 15:41:45,057 INFO     Valid 3p HITS1 at step 500: 0.002717\n",
                        "2025-03-20 15:41:45,058 INFO     Valid 3p HITS3 at step 500: 0.004046\n",
                        "2025-03-20 15:41:45,058 INFO     Valid 3p HITS10 at step 500: 0.006362\n",
                        "2025-03-20 15:41:45,059 INFO     Valid 3p num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,060 INFO     Valid 2i MRR at step 500: 0.006948\n",
                        "2025-03-20 15:41:45,061 INFO     Valid 2i HITS1 at step 500: 0.004684\n",
                        "2025-03-20 15:41:45,062 INFO     Valid 2i HITS3 at step 500: 0.007173\n",
                        "2025-03-20 15:41:45,067 INFO     Valid 2i HITS10 at step 500: 0.009173\n",
                        "2025-03-20 15:41:45,067 INFO     Valid 2i num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,068 INFO     Valid 3i MRR at step 500: 0.006669\n",
                        "2025-03-20 15:41:45,069 INFO     Valid 3i HITS1 at step 500: 0.004450\n",
                        "2025-03-20 15:41:45,070 INFO     Valid 3i HITS3 at step 500: 0.006343\n",
                        "2025-03-20 15:41:45,072 INFO     Valid 3i HITS10 at step 500: 0.009474\n",
                        "2025-03-20 15:41:45,073 INFO     Valid 3i num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,074 INFO     Valid ip MRR at step 500: 0.005277\n",
                        "2025-03-20 15:41:45,075 INFO     Valid ip HITS1 at step 500: 0.002953\n",
                        "2025-03-20 15:41:45,076 INFO     Valid ip HITS3 at step 500: 0.005579\n",
                        "2025-03-20 15:41:45,077 INFO     Valid ip HITS10 at step 500: 0.007461\n",
                        "2025-03-20 15:41:45,078 INFO     Valid ip num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,079 INFO     Valid pi MRR at step 500: 0.004908\n",
                        "2025-03-20 15:41:45,080 INFO     Valid pi HITS1 at step 500: 0.002719\n",
                        "2025-03-20 15:41:45,081 INFO     Valid pi HITS3 at step 500: 0.005262\n",
                        "2025-03-20 15:41:45,082 INFO     Valid pi HITS10 at step 500: 0.006808\n",
                        "2025-03-20 15:41:45,082 INFO     Valid pi num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,085 INFO     Valid 2in MRR at step 500: 0.001597\n",
                        "2025-03-20 15:41:45,085 INFO     Valid 2in HITS1 at step 500: 0.000760\n",
                        "2025-03-20 15:41:45,087 INFO     Valid 2in HITS3 at step 500: 0.001100\n",
                        "2025-03-20 15:41:45,088 INFO     Valid 2in HITS10 at step 500: 0.002040\n",
                        "2025-03-20 15:41:45,089 INFO     Valid 2in num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,090 INFO     Valid 3in MRR at step 500: 0.001459\n",
                        "2025-03-20 15:41:45,092 INFO     Valid 3in HITS1 at step 500: 0.000741\n",
                        "2025-03-20 15:41:45,093 INFO     Valid 3in HITS3 at step 500: 0.001089\n",
                        "2025-03-20 15:41:45,094 INFO     Valid 3in HITS10 at step 500: 0.001355\n",
                        "2025-03-20 15:41:45,094 INFO     Valid 3in num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,095 INFO     Valid inp MRR at step 500: 0.003381\n",
                        "2025-03-20 15:41:45,097 INFO     Valid inp HITS1 at step 500: 0.001849\n",
                        "2025-03-20 15:41:45,098 INFO     Valid inp HITS3 at step 500: 0.003295\n",
                        "2025-03-20 15:41:45,098 INFO     Valid inp HITS10 at step 500: 0.004817\n",
                        "2025-03-20 15:41:45,099 INFO     Valid inp num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,100 INFO     Valid pin MRR at step 500: 0.001231\n",
                        "2025-03-20 15:41:45,102 INFO     Valid pin HITS1 at step 500: 0.000427\n",
                        "2025-03-20 15:41:45,102 INFO     Valid pin HITS3 at step 500: 0.000675\n",
                        "2025-03-20 15:41:45,103 INFO     Valid pin HITS10 at step 500: 0.001575\n",
                        "2025-03-20 15:41:45,105 INFO     Valid pin num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,107 INFO     Valid pni MRR at step 500: 0.001309\n",
                        "2025-03-20 15:41:45,108 INFO     Valid pni HITS1 at step 500: 0.000600\n",
                        "2025-03-20 15:41:45,109 INFO     Valid pni HITS3 at step 500: 0.000887\n",
                        "2025-03-20 15:41:45,110 INFO     Valid pni HITS10 at step 500: 0.001587\n",
                        "2025-03-20 15:41:45,110 INFO     Valid pni num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,111 INFO     Valid 2u-DNF MRR at step 500: 0.003239\n",
                        "2025-03-20 15:41:45,112 INFO     Valid 2u-DNF HITS1 at step 500: 0.001476\n",
                        "2025-03-20 15:41:45,113 INFO     Valid 2u-DNF HITS3 at step 500: 0.002882\n",
                        "2025-03-20 15:41:45,114 INFO     Valid 2u-DNF HITS10 at step 500: 0.005031\n",
                        "2025-03-20 15:41:45,115 INFO     Valid 2u-DNF num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,116 INFO     Valid up-DNF MRR at step 500: 0.003038\n",
                        "2025-03-20 15:41:45,117 INFO     Valid up-DNF HITS1 at step 500: 0.001842\n",
                        "2025-03-20 15:41:45,121 INFO     Valid up-DNF HITS3 at step 500: 0.002574\n",
                        "2025-03-20 15:41:45,121 INFO     Valid up-DNF HITS10 at step 500: 0.003528\n",
                        "2025-03-20 15:41:45,122 INFO     Valid up-DNF num_queries at step 500: 5000.000000\n",
                        "2025-03-20 15:41:45,123 INFO     Valid average MRR at step 500: 0.016468\n",
                        "2025-03-20 15:41:45,124 INFO     Valid average HITS1 at step 500: 0.013084\n",
                        "2025-03-20 15:41:45,125 INFO     Valid average HITS3 at step 500: 0.017580\n",
                        "2025-03-20 15:41:45,127 INFO     Valid average HITS10 at step 500: 0.020638\n",
                        "2025-03-20 15:41:45,128 INFO     Evaluating on Test Dataset...\n",
                        "  0%|          | 0/21951 [00:00<?, ?it/s]2025-03-20 15:41:45,489 INFO     Evaluating the model... (0/21951)\n",
                        "  5%|▍         | 999/21951 [00:24<08:27, 41.31it/s]2025-03-20 15:42:09,761 INFO     Evaluating the model... (1000/21951)\n",
                        "  9%|▉         | 1999/21951 [00:48<08:06, 41.03it/s]2025-03-20 15:42:33,996 INFO     Evaluating the model... (2000/21951)\n",
                        " 14%|█▎        | 2999/21951 [01:12<07:34, 41.67it/s]2025-03-20 15:42:58,151 INFO     Evaluating the model... (3000/21951)\n",
                        " 18%|█▊        | 3999/21951 [01:36<07:10, 41.74it/s]2025-03-20 15:43:22,136 INFO     Evaluating the model... (4000/21951)\n",
                        " 23%|██▎       | 4999/21951 [02:00<06:44, 41.93it/s]2025-03-20 15:43:46,049 INFO     Evaluating the model... (5000/21951)\n",
                        " 27%|██▋       | 5998/21951 [02:25<06:36, 40.21it/s]2025-03-20 15:44:10,238 INFO     Evaluating the model... (6000/21951)\n",
                        " 32%|███▏      | 6999/21951 [02:49<06:27, 38.61it/s]2025-03-20 15:44:35,120 INFO     Evaluating the model... (7000/21951)\n",
                        " 36%|███▋      | 7999/21951 [03:15<05:56, 39.18it/s]2025-03-20 15:45:00,760 INFO     Evaluating the model... (8000/21951)\n",
                        " 41%|████      | 8999/21951 [03:40<05:26, 39.73it/s]2025-03-20 15:45:26,057 INFO     Evaluating the model... (9000/21951)\n",
                        " 46%|████▌     | 9997/21951 [04:06<05:09, 38.67it/s]2025-03-20 15:45:51,608 INFO     Evaluating the model... (10000/21951)\n",
                        " 50%|█████     | 10997/21951 [04:32<04:43, 38.57it/s]2025-03-20 15:46:17,543 INFO     Evaluating the model... (11000/21951)\n",
                        " 55%|█████▍    | 11997/21951 [04:58<04:26, 37.35it/s]2025-03-20 15:46:43,825 INFO     Evaluating the model... (12000/21951)\n",
                        " 59%|█████▉    | 12997/21951 [05:24<03:53, 38.33it/s]2025-03-20 15:47:10,218 INFO     Evaluating the model... (13000/21951)\n",
                        " 64%|██████▍   | 13997/21951 [05:50<03:19, 39.79it/s]2025-03-20 15:47:35,696 INFO     Evaluating the model... (14000/21951)\n",
                        " 66%|██████▌   | 14454/21951 [06:02<03:11, 39.24it/s]"
                    ]
                }
            ],
            "source": [
                "args = Args(\n",
                "    cuda=True,  \n",
                "    do_train=True,  \n",
                "    do_test=True,  \n",
                "    data_path=\"/kaggle/input/kg-data/FB15k-237-betae\",  \n",
                "    negative_sample_size=128,  \n",
                "    batch_size=512,  \n",
                "    hidden_dim=800,  \n",
                "    gamma=60.0,  \n",
                "    learning_rate=0.0001,  \n",
                "    max_steps=1000,  \n",
                "    cpu_num=3,  \n",
                "    test_batch_size=4,  \n",
                "    geo='laplace',  # Changed from 'gamma' to 'laplace' for LapE  \n",
                "    drop=0.1,  \n",
                "    valid_steps=500,  \n",
                "    gamma_mode=\"(1600,4)\",  # Using GammaE settings as reference  \n",
                "    seed=42  \n",
                ")\n",
                "\n",
                "main(args)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
